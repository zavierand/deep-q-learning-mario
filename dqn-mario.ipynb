{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b26059-4484-4c0d-98d0-a5eee92d0b48",
   "metadata": {},
   "source": [
    "# Deep Q-Learning on Mario Bros\n",
    "<hr>\n",
    "This is a reinforcement learning project on implementing a deep q-network and training it to play the Atari 2600 version of Mario Bros. Below is the notebook documenting training, experiments done, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee4ed1-2f42-49c6-ac42-ccbbdfb1515c",
   "metadata": {},
   "source": [
    "Let's create the gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8a435b-faa8-4716-8be0-7fbe7e1f4b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/anaconda3/lib/python3.11/site-packages (0.19.11)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (2.27.0)\n",
      "Requirement already satisfied: setproctitle in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/anaconda3/lib/python3.11/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153bdbd3-b480-4be4-8923-104396ac806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import environment dependencies\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from ale_py import ALEInterface\n",
    "\n",
    "# import saved model\n",
    "import torch\n",
    "\n",
    "# import model\n",
    "from dqn import DQN\n",
    "\n",
    "# import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import wandb for logging\n",
    "import wandb\n",
    "\n",
    "# import rendering\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f889745-eb0a-4b4e-a23b-f01f74a67a50",
   "metadata": {},
   "source": [
    "Here is some starter code to play around with the environment and API - get a feel for what is happening with the API and the wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35cab364-7559-4e6c-bc86-6988647250f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Box(0, 255, (210, 160, 3), uint8)\n",
      "False\n",
      "<class 'numpy.int64'>\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'bool'>\n",
      "render type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.0+dfae0bd)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAGhCAYAAADY5IdbAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMPVJREFUeJzt3Xt0VOW9N/DvnpnM5DaXTG6TQMIdQYEUUGNqvVAiED1UheNRpG/x8kJtUVehtjZdR/HSNlRb26XlqOfUA7aKKL4CB1rxIJcgJUQMUEQxEgwJkEwCCZnJJHPfz/tHZMOYe/ZMJgPfz1rPktnPs/f+ZRu+7NvsLQkhBIiIaEA00S6AiCiWMUSJiFRgiBIRqcAQJSJSgSFKRKQCQ5SISAWGKBGRCgxRIiIVGKJERCowRImIVIhqiK5atQojR45EfHw88vPz8fHHH0ezHCKifotaiL799ttYvnw5VqxYgQMHDiAvLw+zZ89GY2NjtEoiIuo3KVoPIMnPz8c111yDP/3pTwAAWZaRk5ODRx55BL/4xS96nFeWZdTV1cFoNEKSpMEol4guM0IItLa2Ijs7GxpN9/ubukGsSeHz+VBRUYHi4mJlmkajQWFhIcrKyjqN93q98Hq9yufTp0/jyiuvHJRaiejydvLkSQwfPrzb/qgczp89exbBYBCZmZkh0zMzM2G32zuNLykpgdlsVhoDlIgGi9Fo7LE/Jq7OFxcXw+FwKO3kyZPRLomILhO9nTKMyuF8WloatFotGhoaQqY3NDTAZrN1Gm8wGGAwGAarPCKiPovKnqher8f06dOxfft2ZZosy9i+fTsKCgqiURIR0YBEZU8UAJYvX45Fixbh6quvxrXXXos//vGPaGtrw/333x+tkoiI+i1qIXr33XfjzJkzePLJJ2G32/Gtb30LW7du7XSxiYhoKIvafaJqOJ1OmM3maJdBRJcBh8MBk8nUbX9MXJ0nIhqqGKJERCowRImIVGCIEhGpwBAlIlKBIUpEpAJDlIhIBYYoEZEKDFEiIhUYokREKjBEiYhUYIgSEanAECUiUiFqj8IjCgeDLgV6XccTdmThR7vXDgG51/n0OjMMOgsAQIgg2nx2CBHodb44bTLi41I75oOMdm8DZOHrdT6dJgEJ+oyvPwm0+xoRlD29zkdDH0OUYtrojDswMm0OAKDNV4+yY0/AH2ztdb4c63cx3nYPAMAbcGBf1RNo9zX0MhdgM+dj0vAlAICA7EX58afgdH/V63zW5KswbeRjkCBBQMYn1StxtvWfvc5HQx9DlGJKnDYJ6cZpcPvP4lzbUWg1eui0CTjTeghO94lu9ya1GgMyTNPhD7hw1nUYGo0eOm0SmlyfotVTi0A3e4WSpEOGcRoEZJxxHoAk6aDTJqGlvRJOdw0CwbZuKpWQbvwWtBoDGp0V0EhaxGmT4HSfgNP9Fbx+R5i2CEUbQ5RiSnxcOvJyH4HdsQ/n2o4CAGQRRGX9W8rnrsRpjZg0/IdoddegyXXk66kCXzVugt2xr9v5dBoDJg67D7LsQ5PrU2V6zdn/RW3TB93Op5G0GJ91D+LjrGiuvFBXXcseHLO/3ceflmIBQ5RizzfeYCtJWozOuB1e/w0h0+2Oj3G29VCPC8pNm4U045SQqWdbP4XdUdZjCcNSboQpYUTItJb2Kpxq3tFjsZmma2DQhb6VweU5hRNn3wcQcy+ZIDBE6RIgQYMs83UQADSSTnlPuDfgQLPrM8g9XDDKME6HMMrQSHEXvV9cg0bnJz3Ol2qcBGvylV+vr+Mml9PNpag79xGECHY7nyVpPMyJY0LmO+M8hJPNOyDLvj5dFKOhhbc4UcwTIoBPT72KgzUvhJyjHJE6G9eNfRaWxLHdzYkv6v+KT74qgcffpEzNtnwbBeN+jTRjXrfrPN6wEeXHn4bLe1qZlmb8FgrG/RpZlm93O19t0/9iX9UTaGk/pkyzJI5FwdhnkZs2qw8/LQ013BOlmKTTJiLJkIU4bRIEBJzuE/D4m+DynkZcwKiMM8RZoJEu/JprNQYkGbKh13aMcXlOorntKFyeUwjKF25VMugs0Ep65bNGE4dEfRYMcRYAQJu3DudcR+HynIQE7UXzpUCriVc+S5IWiXobDHFWAIDbdwbNrs/R6q5BnPZCnXqdBTpNQpi2Dg0mhijFpAzTdFiTroROeyGw3L6z2Fe14qLDcgAC8MttMOhSAAApSVfg+iueg1YyKEP8QRc+qS5RDq/PCwTd0Go6gjQ5PgffHv8baCW9svyg8OJQzR8hSdpvzHfhSn9CXCquG/vU12Okr0uSceTUf0GjCf3rF5S9A9waFE0MUYpJWo1eCThZDiAteZKyl3heq7sWbRcdbgMde5QGTceFHSFkpCRNAL4Rnu2eejg9JzrWg451aCRtyAUhc+IY2IIFIfN5fGeVw/Tze7+SpFW+DAAAxvhc2Cyh8/n8DjS3HQUvLMUmhijFPEnSYkL2InwzhI7WvY6qhnd7mhNjM+/qNN9XjZvw2ek/97jOkWm3YmRaUci00+dKceDE73qcb1jKTRiWcmPItDOth1Be9RQEur8gRUMXQ5RiijdwDpX1a2FNmoAsy3cgSdJFh+8d/3W6a3C6eddF94MCgWAbqhrWw5w4FjnWmZAkTaf52r121DZtQ3PbF8p8QdmHrxo3wZw4GiNSZ0OjieuYQ5KU+bz+czhxdmvIxSIhZNSc3YpzbV9iZFoRdNqETvP5g204cebvcLq/4lX5GMYQpZjiCzjwVeNG+FJmwGYpgOjiCNjlqcWxhndCpgVkN6rPbEG6cSqGpdwEqYsbU9p8DahqeDfk1iZZ+FHb9AHM7WMwPGUGIEmd5vP4z+F444aQOwMEZJxq3olE/VEMS7lJCd+L+QMuVJ/ZAo//bH82AQ0xkhBd/RoObU6nE2azufeBdMky6FI63ex+njfggNNd3WVfnNYIc+KYb96vDwDwBV1wtB9HV+cmdZoEWBLHdbr4BAAB2YOWtmNdHo5rJD1SksaH3CFwXlD40dL2JWTh77JWGhocDgdMJlO3/QxRIqIe9BaivNmeiEgFhigRkQoMUSIiFRiiREQqMESJiFQIe4iWlJTgmmuugdFoREZGBu644w5UVlaGjLn55puVm6TPt4ceeijcpRARRVzYb7YvLS3F0qVLcc011yAQCOCXv/wlZs2ahc8//xxJSUnKuMWLF+OZZ55RPicmJoanAAnQxmshabq6E3Dggu4ghBxzd4MRXTIkjQRtgrb3gf0gZIGgW93XbcMeolu3bg35vGbNGmRkZKCiogI33njhO8OJiYmw2WzhXj20CVqMvmc09BZ974P7SAQFTrx7Au2n28O2TCLqn4TsBIycPxKSNnw7SN5zXlSvq1YVpBH/2qfD0fFCLqvVGjL9zTffxBtvvAGbzYa5c+fiiSee6HZv1Ov1wuu98Jgwp9PZ7fokSYLerIfBauh2TH/JARmSLrx7tkTUP5o4DfQpemh04TsLKWQR+ujEAYhoiMqyjJ/85Ce4/vrrMWnSJGX6vffeixEjRiA7OxuHDx/G448/jsrKSrz33ntdLqekpARPP/10JEslIhqQiIbo0qVLceTIEezZsydk+pIlS5Q/T548GVlZWZg5cyaOHz+OMWPGdFpOcXExli9frnx2Op3IycmJXOFERH0UsRB9+OGHsWXLFuzevRvDhw/vcWx+fj4AoKqqqssQNRgMMBjCd3hORBQuYQ9RIQQeeeQRbNiwAbt27cKoUaN6nefQoUMAgKysrHCXQ0QUUWEP0aVLl2Lt2rXYtGkTjEYj7HY7AMBsNiMhIQHHjx/H2rVrceuttyI1NRWHDx/GsmXLcOONN2LKlCm9LJ2IaGgJe4i+/PLLADpuqL/Y6tWrcd9990Gv1+PDDz/EH//4R7S1tSEnJwfz58/Hv//7v4e7FCKiiIvI4XxPcnJyUFpaGu7VEhFFBb87T0SkAkOUiEgFhigRkQoMUSIiFRiiREQqMESJiFRgiBIRqcAQJSJSgSFKRKQCQ5SISAWGKBGRCgxRIiIVGKJERCpE/EV1sUIIAb/TDzkgd+4LCgg/X5dMFE2yX4a32dvl2z41Og3iTHGqXzo3EAzRr8l+GTXv1cBd7+7UJyAQ9Kh7NzURqeOuc+PLP38JCZ2DMiE7AWMWjoEUxxCNqqA7iEB7INplEFEXhCwQbO96ZyboDgJROljkOVEiIhUYokREKjBEiYhUYIgSEanAECUiUoEhSkSkAkOUiEgFhigRkQoxfbN9xncyoNGF/jug0WugTdBGqSIiiiW6RB0yrs+A7O/8dW85IKNxT2Pvy4hEYYMl6+YsaOMZmEQ0MLpEHWw32brsC3qCfQpRHs4TEanAECUiUoEhSkSkAkOUiEgFhigRkQoMUSIiFRiiREQqhD1En3rqKUiSFNImTJig9Hs8HixduhSpqalITk7G/Pnz0dDQEO4yiIgGRUT2RK+66irU19crbc+ePUrfsmXLsHnzZqxfvx6lpaWoq6vDvHnzIlEGEVHEReQbSzqdDjZb528BOBwOvPbaa1i7di2++93vAgBWr16NiRMnYt++fbjuuusiUQ4RUcREZE/02LFjyM7OxujRo7Fw4ULU1tYCACoqKuD3+1FYWKiMnTBhAnJzc1FWVtbt8rxeL5xOZ0gjIhoKwh6i+fn5WLNmDbZu3YqXX34Z1dXVuOGGG9Da2gq73Q69Xg+LxRIyT2ZmJux2e7fLLCkpgdlsVlpOTk64yyYiGpCwH84XFRUpf54yZQry8/MxYsQIvPPOO0hISBjQMouLi7F8+XLls9PpZJAS0ZAQ8VucLBYLxo8fj6qqKthsNvh8PrS0tISMaWho6PIc6nkGgwEmkymkERENBREPUZfLhePHjyMrKwvTp09HXFwctm/frvRXVlaitrYWBQUFkS6FiCjswn44/9hjj2Hu3LkYMWIE6urqsGLFCmi1WixYsABmsxkPPvggli9fDqvVCpPJhEceeQQFBQW8Mk9EMSnsIXrq1CksWLAATU1NSE9Px3e+8x3s27cP6enpAIA//OEP0Gg0mD9/PrxeL2bPno3/+I//CHcZRESDIuwhum7duh774+PjsWrVKqxatSrcqyYiGnT87jwRkQox/Y6l5k+boYkLz78DIiAQcAfCsiwiGlyB9gCaP22GpJXCtsyuXl7XFUkIIcK21kHidDphNpujXQYRXQYcDkePt1XG9J5o8be/jXhdTP8Il61qhwN/PXwYwdj7N7xPtJKERXl5GMF7mmOWJxBAyd69vY6L6QT6wZQpMBoM0S6DBmDvyZN489NPL90Q1Whw29ixyB82LNql0AC1er19ClFeWCIiUoEhSkSkAkOUiEiFmD4nuu2rr5AQFxftMmgAvmhqgnyJng8FAFkIlJ8+jWa3O9ql0AC5/f4+jYv5W5zCd1cYDbaY+8XrJ/5uxrbzv5+X9C1OwKX/F5FiF383Lw88J0pEpAJDlIhIBYYoEZEKDFEiIhUYokREKjBEiYhUYIgSEanAECUiUoEhSkSkAkOUiEgFhigRkQoMUSIiFRiiREQqMESJiFRgiBIRqcAQJSJSgSFKRKQCQ5SISAWGKBGRCgxRIiIVGKJERCowRImIVAh7iI4cORKSJHVqS5cuBQDcfPPNnfoeeuihcJdBRDQowv7e+f379yMYDCqfjxw5gltuuQV33XWXMm3x4sV45plnlM+JiYnhLoOIaFCEPUTT09NDPq9cuRJjxozBTTfdpExLTEyEzWYL96qJiAZdRM+J+nw+vPHGG3jggQcgSZIy/c0330RaWhomTZqE4uJitLe397gcr9cLp9MZ0oiIhgQRQW+//bbQarXi9OnTyrRXX31VbN26VRw+fFi88cYbYtiwYeLOO+/scTkrVqwQANjY2NgGvTkcjh7zSRJCCETI7NmzodfrsXnz5m7H7NixAzNnzkRVVRXGjBnT5Riv1wuv16t8djqdyMnJCXu9RETf5HA4YDKZuu0P+znR82pqavDhhx/ivffe63Fcfn4+APQYogaDAQaDIew1EhGpFbFzoqtXr0ZGRgZuu+22HscdOnQIAJCVlRWpUoiIIiYie6KyLGP16tVYtGgRdLoLqzh+/DjWrl2LW2+9FampqTh8+DCWLVuGG2+8EVOmTIlEKUREkRWG60edfPDBBwKAqKysDJleW1srbrzxRmG1WoXBYBBjx44VP/vZz3o9cftNDocj6ieb2djYLo8W1QtLkeJ0OmE2m6NdBhFdBnq7sMTvzhMRqcAQJSJSgSFKRKQCQ5SISAWGKBGRCgxRIiIVGKJERCowRImIVGCIEhGpwBAlIlKBIUpEpAJDlIhIBYYoEZEKDFEiIhUYokREKjBEiYhUYIgSEanAECUiUoEhSkSkAkOUiEgFhigRkQoMUSIiFRiiREQqMESJiFRgiBIRqcAQJSJSQRftAtR45qabEK+L/I8QlGX858GDqG5pifi6zps/YQLyhw0btPUR9dfeU6ewsbJy0NY3JiUF/3fqVGglaVDW5wkE8GRpaa/jYjpE75o4EUaDIeLr8QYC2FBZOaghek12Nu6dNGnQ1kfUX75gcFBDNCMpCXdfeSX0Wu2grK/V6+1TiPJwnohIBYYoEZEKDFEiIhVi+pzo/3z55aBcWAoIgbPt7RFfz8UO2u1IGISfjWigDjU0DOr6zrS1YWNl5aBeWOoLSQghIlxL2DmdTpjN5miXQUSXAYfDAZPJ1G1/vw/nd+/ejblz5yI7OxuSJGHjxo0h/UIIPPnkk8jKykJCQgIKCwtx7NixkDHNzc1YuHAhTCYTLBYLHnzwQbhcrv6WQkQUdf0O0ba2NuTl5WHVqlVd9j/33HN48cUX8corr6C8vBxJSUmYPXs2PB6PMmbhwoX47LPPsG3bNmzZsgW7d+/GkiVLBv5TEBFFi1ABgNiwYYPyWZZlYbPZxPPPP69Ma2lpEQaDQbz11ltCCCE+//xzAUDs379fGfP+++8LSZLE6dOn+7Reh8MhALCxsbFFvDkcjh7zKKxX56urq2G321FYWKhMM5vNyM/PR1lZGQCgrKwMFosFV199tTKmsLAQGo0G5eXlXS7X6/XC6XSGNCKioSCsIWq32wEAmZmZIdMzMzOVPrvdjoyMjJB+nU4Hq9WqjPmmkpISmM1mpeXk5ISzbCKiAYuJ+0SLi4vhcDiUdvLkyWiXREQEIMwharPZAAAN37h/rKGhQemz2WxobGwM6Q8EAmhublbGfJPBYIDJZAppRERDQVhDdNSoUbDZbNi+fbsyzel0ory8HAUFBQCAgoICtLS0oKKiQhmzY8cOyLKM/Pz8cJZDRBR5/bgYL4QQorW1VRw8eFAcPHhQABAvvPCCOHjwoKipqRFCCLFy5UphsVjEpk2bxOHDh8Xtt98uRo0aJdxut7KMOXPmiKlTp4ry8nKxZ88eMW7cOLFgwYI+18Cr82xsbIPVers63+8Q3blzZ5crWrRokRCi4zanJ554QmRmZgqDwSBmzpwpKisrQ5bR1NQkFixYIJKTk4XJZBL333+/aG1tZYiysbENudZbiPJrn0REPQj71z6JiOgChigRkQoMUSIiFRiiREQqMESJiFRgiBIRqcAQJSJSgSFKRKQCQ5SISAWGKBGRCgxRIiIVGKJERCowRImIVGCIEhGpwBAlIlKBIUpEpAJDlIhIBYYoEZEKDFEiIhUYokREKjBEiYhUYIgSEanAECUiUoEhSkSkAkOUiEgFhigRkQoMUSIiFRiiREQqMESJiFRgiBIRqcAQJSJSgSFKRKQCQ5SISIV+h+ju3bsxd+5cZGdnQ5IkbNy4Uenz+/14/PHHMXnyZCQlJSE7Oxs/+MEPUFdXF7KMkSNHQpKkkLZy5UrVPwwR0WDrd4i2tbUhLy8Pq1at6tTX3t6OAwcO4IknnsCBAwfw3nvvobKyEt/73vc6jX3mmWdQX1+vtEceeWRgPwERURTp+jtDUVERioqKuuwzm83Ytm1byLQ//elPuPbaa1FbW4vc3FxlutFohM1m6+/qiYiGlIifE3U4HJAkCRaLJWT6ypUrkZqaiqlTp+L5559HIBDodhlerxdOpzOkERENBf3eE+0Pj8eDxx9/HAsWLIDJZFKmP/roo5g2bRqsViv27t2L4uJi1NfX44UXXuhyOSUlJXj66acjWSoR0cAIFQCIDRs2dNnn8/nE3LlzxdSpU4XD4ehxOa+99prQ6XTC4/F02e/xeITD4VDayZMnBQA2Nja2iLfe8isie6J+vx//9m//hpqaGuzYsSNkL7Qr+fn5CAQCOHHiBK644opO/QaDAQaDIRKlEhGpEvYQPR+gx44dw86dO5GamtrrPIcOHYJGo0FGRka4yyEiiqh+h6jL5UJVVZXyubq6GocOHYLVakVWVhb+9V//FQcOHMCWLVsQDAZht9sBAFarFXq9HmVlZSgvL8eMGTNgNBpRVlaGZcuW4fvf/z5SUlLC95MREQ2GPp38vMjOnTu7PG+waNEiUV1d3e15hZ07dwohhKioqBD5+fnCbDaL+Ph4MXHiRPGb3/ym2/OhXXE4HFE/T8LGxnZ5tN7OiUpCCIEY43Q6YTabo10GEV0GHA5Hj9d1+N15IiIVGKJERCowRImIVGCIEhGpwBAlIlKBIUpEpAJDlIhIBYYoEZEKEX0UXqRNSkuDVsN/B8LhrNuN062t0S6DVBhuNCI1ISHaZVwygrKMI2fP9joupkP0L3fcgWS9PtplXBLe+PRTPPPRR9Eug1RYMm0a7rnqqmiXcclw+XyY9uc/9zoupkM0QadDYlxctMu4JOi12miXQCrptVr+fQijoCz3aRyPhYmIVGCIEhGpENOH8+1+PzSSFO0yLgneYDDaJZBK3mAQbT5ftMu4ZLT7/X0aF9OPwpuYmsqr82HS7HajzuWKdhmkQrbRCGt8fLTLuGQEZRlHm5p6fRReTIcoEVGk8XmiREQRxBAlIlKBIUpEpEJMX52ny0+6xYCFs0fCoB+cf/89XhlvfnACZx3eQVkfxR6G6BAjQQON5vz/FoGgHEDHSwcJACxGPe6amQtj4uB8M8fZ5seWf5xmiFK3GKJDTE76dZg6+v8AAIKyD3s+/wNa2k5Etygi6hZDdIjQaOKQbEhHavIY2CxTIEkS/EE39Do+lYdoKGOIDhGWpFwUTXsOhjgzJH4LiyhmMESjTJK0yE6ZijTTOCTordBpDUqfRtIi2zoNSfEZAACXpxFnHEejVSoRdYEhGmVx2ngUTHgEqcYxAEL3QDVSHPLH/xjnLyxV1W/H9sMrBr9IIuoWQzTKArIPR2rfRYI+JWT67dePxTVX2AAAZxxu/Ofmf6Ku+Vg0SiSiHjBEo0yW/fji1GYAgE6rgSwEZFlg8fduwaz8KQjKAkdrm/D5qXVocfE2G6KhhiE6RIwdZsHzP7wZuw+fwh/e/QQA4PUH8firpThwrAEud98ey0VEg4shOkQYE/W4+Vu5OOtwK9NkIXCiwYGTZ1phsyYp01tcHoYq0RDBEB3C4vU6/Ndjc+APhD4w+Vd/LcN//e1wlKoioosxRIcwIQt8VdeCQFBG/sQsxOk6XiY3fbwN1XYHKr5swLlWT5SrHFxtngDKjzQhwTA4L9Zr9wbQ7gkMyrooNjFEhzCPP4jHXtmJ1nYfdv1hAVKMHcFxf9FkLJg5Ef9S/P/w0aenolzl4Ko748ZjfzqIwfo6ggAgy3x2AXWv34/C2b17N+bOnYvs7GxIkoSNGzeG9N93332QJCmkzZkzJ2RMc3MzFi5cCJPJBIvFggcffBAuvpoCAJA3JgO/XHgdpo3LBAAEgwIN59rx+3f249dvlOHXb5Rh5dp9+N3bH+NkozPK1UaHLAsEB6kxQKk3/d4TbWtrQ15eHh544AHMmzevyzFz5szB6tWrlc8GgyGkf+HChaivr8e2bdvg9/tx//33Y8mSJVi7dm1/y7nkTBufiWnjOwK0zeOHRiOh2enGc+vKlTH8e000dPQ7RIuKilBUVNTjGIPBAJvN1mXf0aNHsXXrVuzfvx9XX301AOCll17Crbfeit/97nfIzs7ub0mXLEOcFiWLb0RLa+j9oWs+OIL/2VsVpaqI6GIROSe6a9cuZGRkICUlBd/97nfxq1/9CqmpqQCAsrIyWCwWJUABoLCwEBqNBuXl5bjzzjs7Lc/r9cLrvRAkTueldxgbCMg442iHEAKmpI49d51Wgxun5ChjPL4AHG1e/OOz00i3JKDF5YU/IEerZCJCBF4PMmfOHPzlL3/B9u3b8dvf/halpaUoKipC8Ov3mtvtdmRkZITMo9PpYLVaYbfbu1xmSUkJzGaz0nJycrocF8sqT57DrMfeQcnacnT3AtbtB2pww6NrkWZKwM4X7sHUsRldjiOiwRP2PdF77rlH+fPkyZMxZcoUjBkzBrt27cLMmTMHtMzi4mIsX75c+ex0Oi+5IPUFgjjR4MSR6jP46NNTGDcsBVmpyQCAdo8fh4434pNKO76qd0CrlTAqy4J4PW+uIIq2iL+oZvTo0UhLS0NVVcc5PJvNhsbGxpAxgUAAzc3N3Z5HNRgMMJlMIe1S9cH+E5jz+Lv4e/lXyrTaRifuemoTStaW9zAnEUVDxHdlTp06haamJmRlZQEACgoK0NLSgoqKCkyfPh0AsGPHDsiyjPz8/EiXM2SZDVrcMNwI3dcPZE5yOHHinx1B6jzXhpnDktGe2nGuNNXdjprD1fC0XV432hMNRf0OUZfLpexVAkB1dTUOHToEq9UKq9WKp59+GvPnz4fNZsPx48fx85//HGPHjsXs2bMBABMnTsScOXOwePFivPLKK/D7/Xj44Ydxzz33XNZX5tMTdLhvUgYSdF8fHLS0oHJvi9K/8Aqr8mfhcuLLMgfaHW2DXCURfZMkuruK0Y1du3ZhxowZnaYvWrQIL7/8Mu644w4cPHgQLS0tyM7OxqxZs/Dss88iMzNTGdvc3IyHH34Ymzdvhkajwfz58/Hiiy8iOTm5TzU4nU6Yzeb+lD1k6bUS7p2YhtFmA/IykqDT9P5dnJ21DpTXu3D4TDsc3mCv44lo4BwOR4+nEPsdokPBpRSiiToNnrs5F6PM8QAAf1CGJygQr5UQpw09Ze2XBTwBGW98dgZbvmqJQrVEl5/eQpSXd4eYj+1t+OtnZ7DwyjTcMDz0f9w/G9vw2uFGnPPygRhEQ0XEr85T/7T5gzjZ6oPL1/kwvc0vo7bVB4NWg7GW+AvnT4koavi3MAbdNjoFv7ohB6PMht4HE1FE8XA+ygKywJ5TrfiiqeOJ9nWeDEwcng85wYQaf2LI2LPB4wDq0IoxaBST4MffALg7L5SIBg1DNMp8ssC6L5qUz1cMuwY3T/o5ZEnCYd+FcUII1Aa2A/gQ9YGr8anvLrjkCgD1g14zEV3AEI0BbZ6zOFj9V+g0Btx01S+QYbkq2iUR0dd4TnQI0GkM0GkTOpomrlO/P9iG6oZStHubMWH4XKQax0ShSiLqCvdEo0ynTcDNk4phThwOAIjXmyFJoTfcJ8fbUDTteeh1SV0tgoiiiCEaZRpJg5TkkUg1ju3UJ4QMp7sOgaAXkqSBP+hGs6vj+/SyCCAQ5EUlomhjiA5hQdmH3Ud+izPOyi56BfxBPoCEKNoYokOEx+fAqab9kMWFbyMFZT9a3fXwBfgSP6KhiiE6RLS661F6pAR+HqITxZSYDtHsW7KhiQvPDQYiKNC4rxF+hz8syxtQDVFbM1Fs05v1SC9Ih9SHp6D1leyXUbetrtdxMR2i6fnp0MZrw7KsoC+Ic5+eG/QQFeg4bJdlPlSEaKB0yTqkX5sOjT58d20GPcFLP0QvBYGAG6VHVgIQCAa9vY4noqGFIRplAjKaWo9FuwwiGiB+Y4mISAWGKBGRCgxRIiIVGKJERCowRImIVGCIEhGpwBAlIlKBIUpEpAJDlIhIBYYoEZEKDFEiIhUYokREKjBEiYhUYIgSEanAECUiUoEhSkSkAkOUiEiFfj/Zfvfu3Xj++edRUVGB+vp6bNiwAXfccYfSL0ldvyjqueeew89+9jMAwMiRI1FTUxPSX1JSgl/84hf9qqVxX2P4XlQXEPC7oveSOiIaOL/Lj8Z9jZC04X1RXV/0O0Tb2tqQl5eHBx54APPmzevUX19fH/L5/fffx4MPPoj58+eHTH/mmWewePFi5bPRaOxvKaj/sL73QUR0yfM7/Kj7395fKhcJ/Q7RoqIiFBUVddtvs9lCPm/atAkzZszA6NGjQ6YbjcZOY/vr0ek2GLQ8I0FE4ecNynixwt7ruIi+qK6hoQF/+9vf8Prrr3fqW7lyJZ599lnk5ubi3nvvxbJly6DTdV2O1+uF13vhTZhOpxMAcMMwIxLjwvPKZCKii7X7g9EP0ddffx1Go7HTYf+jjz6KadOmwWq1Yu/evSguLkZ9fT1eeOGFLpdTUlKCp59+OpKlEhENiCSEEAOeWZI6XVi62IQJE3DLLbfgpZde6nE5//3f/40f/vCHcLlcMBgMnfq72hPNycnB+u+N454oEUVEuz+Iu/7nGBwOB0wmU7fjIrYn+tFHH6GyshJvv/12r2Pz8/MRCARw4sQJXHHFFZ36DQZDl+FKRBRtEQvR1157DdOnT0deXl6vYw8dOgSNRoOMjIx+rWPbiRbo+3FhSaeRcF22EUY9916JKDz6HaIulwtVVVXK5+rqahw6dAhWqxW5ubkAOg63169fj9///ved5i8rK0N5eTlmzJgBo9GIsrIyLFu2DN///veRkpLSr1r+8/CZfo1PitNgfEoCQ5SIwqbfIfrJJ59gxowZyufly5cDABYtWoQ1a9YAANatWwchBBYsWNBpfoPBgHXr1uGpp56C1+vFqFGjsGzZMmU5/fHDvIx+3eKk00hITYjotTQiusyourAULU6nE2azmReWLmcaCZKu639AhSyAQDffNpEA6Zu/MwIQ/mCYC6RYF/ULS0SRpB+XiuSZY7vs833VDNfWL7vsixtuhvFfJgAXfT1Zdvvh/H9HIDu9Xc5D1BOGKMUWnQbalATobEboso2QW30Q7b4LfdZEyG4/dJnJCLZ6Idq/fh6CRoLWen4+EyTNhRANurzd7tUS9YYhSjFFa01AyqJp0CTqAQDte2vg3n/qQt9906EflYKUxdegbcdxtO+tBQBokvWwLPwWNJZ4IHzPqCBiiFJskSQJUrwO0td3WAh/EMIT6Pizt+O8pqTVQErQABftXQq/DN/xZkhfX1jUZSYjLqv781xEfcUQpcuCcPvRuuUL5XPSzaMYohQWPBFERKQCQ5SISAWGKBGRCgxRIiIVGKJERCrw6jzFFNkbgPeLM9ClJ0GXbUJclhHylR1P/9IYDYBWg6DTA/9JB4Jn2y/MqNNAP9qq3FSvzUhWuiSdBvpxqZBbO27aDza1I9DgGrwfimIaQ5RiitzigXP9EcRPzYJp/iTETx+G+OnDQsb4jjfB8dY/gYueCqFJjIPpjiuhMXV+Lq1k0ME4d6LyuX1vLVx/r4zYz0CXFoYoxaRAXStc26q67AueaQsJUAAQ3gDadldDMvT+Kx845QhHiXSZYIhSTAo0uPp1yC28Qbj3nYxgRXS54oUlIiIVGKJERCowRImIVGCIEhGpwBAlIlKBIUpEpAJDlIhIBYYoEZEKDFEiIhUYokREKjBEiYhUYIgSEanAECUiUoEhSkSkAkOUiEgFhigRkQoMUSIiFWL6yfZV5zyI1/HfASIKP09A7tM4SQgheh82tDidTpjNZug1gCRJ0S6HiC5BQgj4ZMDhcMBkMvU4sM9+85vfiKuvvlokJyeL9PR0cfvtt4svvvgiZIzb7RY//vGPhdVqFUlJSWLevHnCbreHjKmpqRG33nqrSEhIEOnp6eKxxx4Tfr+/z3U4HA6BjleRsbGxsUW0ORyOHvOoX8fCpaWlWLp0Kfbt24dt27bB7/dj1qxZaGtrU8YsW7YMmzdvxvr161FaWoq6ujrMmzdP6Q8Gg7jtttvg8/mwd+9evP7661izZg2efPLJ/pRCRDQ09H0/tLPGxkYBQJSWlgohhGhpaRFxcXFi/fr1ypijR48KAKKsrEwIIcTf//53odFoQvZOX375ZWEymYTX6+3TerknysbGNlgtrHui3+RwdLyf22q1AgAqKirg9/tRWFiojJkwYQJyc3NRVlYGACgrK8PkyZORmZmpjJk9ezacTic+++yzLtfj9XrhdDpDGhHRUDDgEJVlGT/5yU9w/fXXY9KkSQAAu90OvV4Pi8USMjYzMxN2u10Zc3GAnu8/39eVkpISmM1mpeXk5Ay0bCKisBpwiC5duhRHjhzBunXrwllPl4qLi+FwOJR28uTJiK+TiKgvBnSf6MMPP4wtW7Zg9+7dGD58uDLdZrPB5/OhpaUlZG+0oaEBNptNGfPxxx+HLK+hoUHp64rBYIDBYBhIqUREkdWfC0myLIulS5eK7Oxs8eWXX3bqP39h6d1331WmffHFFwLofGGpoaFBGfPqq68Kk8kkPB5Pn+rghSU2NrbBar1dWOpXiP7oRz8SZrNZ7Nq1S9TX1yutvb1dGfPQQw+J3NxcsWPHDvHJJ5+IgoICUVBQoPQHAgExadIkMWvWLHHo0CGxdetWkZ6eLoqLi/tcB0OUjY1tsFpYQ7S7laxevVoZc/5m+5SUFJGYmCjuvPNOUV9fH7KcEydOiKKiIpGQkCDS0tLET3/6U95sz8bGNiRbbyEa01/7JCKKtN6+9smndxARqcAQJSJSgSFKRKQCQ5SISAWGKBGRCgxRIiIVGKJERCowRImIVIjJEI3B7wcQUYzqLW9iMkRbW1ujXQIRXSZ6y5uY/NqnLMuorKzElVdeiZMnT/b8Jj4aEKfTiZycHG7fCOH2jaxwbF8hBFpbW5GdnQ2Npvv9zZh877xGo8GwYcMAACaTib+EEcTtG1ncvpGldvv25RkdMXk4T0Q0VDBEiYhUiNkQNRgMWLFiBV8bEiHcvpHF7RtZg7l9Y/LCEhHRUBGze6JEREMBQ5SISAWGKBGRCgxRIiIVGKJERCrEZIiuWrUKI0eORHx8PPLz8/Hxxx9Hu6SY9NRTT0GSpJA2YcIEpd/j8WDp0qVITU1FcnIy5s+fj4aGhihWPLTt3r0bc+fORXZ2NiRJwsaNG0P6hRB48sknkZWVhYSEBBQWFuLYsWMhY5qbm7Fw4UKYTCZYLBY8+OCDcLlcg/hTDF29bd/77ruv0+/znDlzQsZEYvvGXIi+/fbbWL58OVasWIEDBw4gLy8Ps2fPRmNjY7RLi0lXXXUV6uvrlbZnzx6lb9myZdi8eTPWr1+P0tJS1NXVYd68eVGsdmhra2tDXl4eVq1a1WX/c889hxdffBGvvPIKysvLkZSUhNmzZ8Pj8ShjFi5ciM8++wzbtm3Dli1bsHv3bixZsmSwfoQhrbftCwBz5swJ+X1+6623Qvojsn17fCv9EHTttdeKpUuXKp+DwaDIzs4WJSUlUawqNq1YsULk5eV12dfS0iLi4uLE+vXrlWlHjx4VAERZWdkgVRi7AIgNGzYon2VZFjabTTz//PPKtJaWFmEwGMRbb70lhBDi888/FwDE/v37lTHvv/++kCRJnD59etBqjwXf3L5CCLFo0SJx++23dztPpLZvTO2J+nw+VFRUoLCwUJmm0WhQWFiIsrKyKFYWu44dO4bs7GyMHj0aCxcuRG1tLQCgoqICfr8/ZFtPmDABubm53NYDUF1dDbvdHrI9zWYz8vPzle1ZVlYGi8WCq6++WhlTWFgIjUaD8vLyQa85Fu3atQsZGRm44oor8KMf/QhNTU1KX6S2b0yF6NmzZxEMBpGZmRkyPTMzE3a7PUpVxa78/HysWbMGW7duxcsvv4zq6mrccMMNaG1thd1uh16vh8ViCZmH23pgzm+znn537XY7MjIyQvp1Oh2sViu3eR/MmTMHf/nLX7B9+3b89re/RWlpKYqKihAMBgFEbvvG5KPwKDyKioqUP0+ZMgX5+fkYMWIE3nnnHSQkJESxMqL+u+eee5Q/T548GVOmTMGYMWOwa9cuzJw5M2Lrjak90bS0NGi12k5XiBsaGmCz2aJU1aXDYrFg/PjxqKqqgs1mg8/nQ0tLS8gYbuuBOb/NevrdtdlsnS6QBgIBNDc3c5sPwOjRo5GWloaqqioAkdu+MRWier0e06dPx/bt25Vpsixj+/btKCgoiGJllwaXy4Xjx48jKysL06dPR1xcXMi2rqysRG1tLbf1AIwaNQo2my1kezqdTpSXlyvbs6CgAC0tLaioqFDG7NixA7IsIz8/f9BrjnWnTp1CU1MTsrKyAERw+w74klSUrFu3ThgMBrFmzRrx+eefiyVLlgiLxSLsdnu0S4s5P/3pT8WuXbtEdXW1+Mc//iEKCwtFWlqaaGxsFEII8dBDD4nc3FyxY8cO8cknn4iCggJRUFAQ5aqHrtbWVnHw4EFx8OBBAUC88MIL4uDBg6KmpkYIIcTKlSuFxWIRmzZtEocPHxa33367GDVqlHC73coy5syZI6ZOnSrKy8vFnj17xLhx48SCBQui9SMNKT1t39bWVvHYY4+JsrIyUV1dLT788EMxbdo0MW7cOOHxeJRlRGL7xlyICiHESy+9JHJzc4VerxfXXnut2LdvX7RLikl33323yMrKEnq9XgwbNkzcfffdoqqqSul3u93ixz/+sUhJSRGJiYnizjvvFPX19VGseGjbuXOnANCpLVq0SAjRcZvTE088ITIzM4XBYBAzZ84UlZWVIctoamoSCxYsEMnJycJkMon7779ftLa2RuGnGXp62r7t7e1i1qxZIj09XcTFxYkRI0aIxYsXd9q5isT25fNEiYhUiKlzokREQw1DlIhIBYYoEZEKDFEiIhUYokREKjBEiYhUYIgSEanAECUiUoEhSkSkAkOUiEgFhigRkQr/HzMPI024goQGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's create the environment to make sure it works\n",
    "env = gym.make(\n",
    "    'ALE/MarioBros-v5', \n",
    "    obs_type = 'rgb',\n",
    "    render_mode = 'rgb_array'\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "ACTION_SIZE = env.action_space.n\n",
    "ACTIONS = env.action_space.sample()\n",
    "OBSERVATIONS = env.observation_space\n",
    "obs, reward, terminated, truncated, info = env.step(ACTIONS)\n",
    "\n",
    "# print and log to see data types\n",
    "print(ACTIONS)\n",
    "print(OBSERVATIONS)\n",
    "print(terminated)\n",
    "\n",
    "print(type(ACTIONS))\n",
    "print(type(OBSERVATIONS))\n",
    "print(type(terminated))\n",
    "\n",
    "# render the image\n",
    "img = plt.imshow(env.render()) # only call this once\n",
    "print('render type:', type(env.render()))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4037d9b5-1a78-4ced-ae94-78bb50e87e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, let's actually start writing some code\n",
    "rew_arr = [] # array holding rewards\n",
    "episode_count = 100 # num episodes to train on\n",
    "\n",
    "# instantiate the mario emulator gym\n",
    "env = gym.make(\n",
    "    'ALE/MarioBros-v5',\n",
    "    obs_type = 'rgb',\n",
    "    render_mode = 'rgb_array',\n",
    "    frameskip = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990f1e09-7b1b-4f6b-a9d6-cdc7f95c3c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzavierand\u001b[0m (\u001b[33mzavierand-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/user/Desktop/Projects/dqn-mario/wandb/run-20250511_033957-0d611bed</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zavierand-new-york-university/dqn-mario/runs/0d611bed' target=\"_blank\">icy-music-28</a></strong> to <a href='https://wandb.ai/zavierand-new-york-university/dqn-mario' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zavierand-new-york-university/dqn-mario' target=\"_blank\">https://wandb.ai/zavierand-new-york-university/dqn-mario</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zavierand-new-york-university/dqn-mario/runs/0d611bed' target=\"_blank\">https://wandb.ai/zavierand-new-york-university/dqn-mario/runs/0d611bed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-music-28</strong> at: <a href='https://wandb.ai/zavierand-new-york-university/dqn-mario/runs/0d611bed' target=\"_blank\">https://wandb.ai/zavierand-new-york-university/dqn-mario/runs/0d611bed</a><br> View project at: <a href='https://wandb.ai/zavierand-new-york-university/dqn-mario' target=\"_blank\">https://wandb.ai/zavierand-new-york-university/dqn-mario</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250511_033957-0d611bed/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/user/Desktop/Projects/dqn-mario/wandb/run-20250511_033958-trz14m0v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zavierand-new-york-university/dqn-mario/runs/trz14m0v' target=\"_blank\">lemon-puddle-29</a></strong> to <a href='https://wandb.ai/zavierand-new-york-university/dqn-mario' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zavierand-new-york-university/dqn-mario' target=\"_blank\">https://wandb.ai/zavierand-new-york-university/dqn-mario</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zavierand-new-york-university/dqn-mario/runs/trz14m0v' target=\"_blank\">https://wandb.ai/zavierand-new-york-university/dqn-mario/runs/trz14m0v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs beginning.....\n",
      "Epoch 1/5000, [------------------------------] Loss: 0.0000, Epsilon: 0.9980, Reward: 0.0\n",
      "Epoch 2/5000, [------------------------------] Loss: 0.0000, Epsilon: 0.9960, Reward: 0.0\n",
      "Epoch 3/5000, [------------------------------] Loss: 0.0000, Epsilon: 0.9940, Reward: 0.0\n",
      "Epoch 4/5000, [------------------------------] Loss: 1.2347, Epsilon: 0.9920, Reward: 800.0\n",
      "Epoch 5/5000, [------------------------------] Loss: 1.9408, Epsilon: 0.9900, Reward: 0.0\n",
      "Epoch 6/5000, [------------------------------] Loss: 1.1382, Epsilon: 0.9881, Reward: 1600.0\n",
      "Epoch 7/5000, [------------------------------] Loss: 0.8991, Epsilon: 0.9861, Reward: 2400.0\n",
      "Epoch 8/5000, [------------------------------] Loss: 1.8298, Epsilon: 0.9841, Reward: 0.0\n",
      "Epoch 9/5000, [------------------------------] Loss: 0.3901, Epsilon: 0.9821, Reward: 0.0\n",
      "Epoch 10/5000, [------------------------------] Loss: 2.6908, Epsilon: 0.9802, Reward: 0.0\n",
      "Epoch 11/5000, [------------------------------] Loss: 1.0130, Epsilon: 0.9782, Reward: 800.0\n",
      "Epoch 12/5000, [------------------------------] Loss: 1.3314, Epsilon: 0.9763, Reward: 0.0\n",
      "Epoch 13/5000, [------------------------------] Loss: 0.4564, Epsilon: 0.9743, Reward: 0.0\n",
      "Epoch 14/5000, [------------------------------] Loss: 1.2058, Epsilon: 0.9724, Reward: 0.0\n",
      "Epoch 15/5000, [------------------------------] Loss: 1.3456, Epsilon: 0.9704, Reward: 1600.0\n",
      "Epoch 16/5000, [------------------------------] Loss: 2.6085, Epsilon: 0.9685, Reward: 0.0\n",
      "Epoch 17/5000, [------------------------------] Loss: 1.3964, Epsilon: 0.9665, Reward: 0.0\n",
      "Epoch 18/5000, [------------------------------] Loss: 0.0253, Epsilon: 0.9646, Reward: 0.0\n",
      "Epoch 19/5000, [------------------------------] Loss: 0.3827, Epsilon: 0.9627, Reward: 0.0\n",
      "Epoch 20/5000, [------------------------------] Loss: 0.8914, Epsilon: 0.9608, Reward: 0.0\n",
      "Epoch 21/5000, [------------------------------] Loss: 0.5654, Epsilon: 0.9588, Reward: 0.0\n",
      "Epoch 22/5000, [------------------------------] Loss: 0.1840, Epsilon: 0.9569, Reward: 0.0\n",
      "Epoch 23/5000, [------------------------------] Loss: 0.1535, Epsilon: 0.9550, Reward: 0.0\n",
      "Epoch 24/5000, [------------------------------] Loss: 0.0710, Epsilon: 0.9531, Reward: 0.0\n",
      "Epoch 25/5000, [------------------------------] Loss: 0.3502, Epsilon: 0.9512, Reward: 1600.0\n",
      "Epoch 26/5000, [------------------------------] Loss: 0.3170, Epsilon: 0.9493, Reward: 0.0\n",
      "Epoch 27/5000, [------------------------------] Loss: 0.5276, Epsilon: 0.9474, Reward: 0.0\n",
      "Epoch 28/5000, [------------------------------] Loss: 0.2842, Epsilon: 0.9455, Reward: 0.0\n",
      "Epoch 29/5000, [------------------------------] Loss: 0.3402, Epsilon: 0.9436, Reward: 0.0\n",
      "Epoch 30/5000, [------------------------------] Loss: 0.2910, Epsilon: 0.9417, Reward: 2400.0\n",
      "Epoch 31/5000, [------------------------------] Loss: 1.2806, Epsilon: 0.9398, Reward: 1600.0\n",
      "Epoch 32/5000, [------------------------------] Loss: 1.2475, Epsilon: 0.9379, Reward: 0.0\n",
      "Epoch 33/5000, [------------------------------] Loss: 19878.2461, Epsilon: 0.9361, Reward: 0.0\n",
      "Epoch 34/5000, [------------------------------] Loss: 0.6396, Epsilon: 0.9342, Reward: 0.0\n",
      "Epoch 35/5000, [------------------------------] Loss: 1.1982, Epsilon: 0.9323, Reward: 0.0\n",
      "Epoch 36/5000, [------------------------------] Loss: 0.7759, Epsilon: 0.9305, Reward: 0.0\n",
      "Epoch 37/5000, [------------------------------] Loss: 0.9191, Epsilon: 0.9286, Reward: 0.0\n",
      "Epoch 38/5000, [------------------------------] Loss: 1.4099, Epsilon: 0.9267, Reward: 0.0\n",
      "Epoch 39/5000, [------------------------------] Loss: 1.2316, Epsilon: 0.9249, Reward: 0.0\n",
      "Epoch 40/5000, [------------------------------] Loss: 0.9070, Epsilon: 0.9230, Reward: 0.0\n",
      "Epoch 41/5000, [------------------------------] Loss: 0.0836, Epsilon: 0.9212, Reward: 0.0\n",
      "Epoch 42/5000, [------------------------------] Loss: 0.5654, Epsilon: 0.9194, Reward: 1600.0\n",
      "Epoch 43/5000, [------------------------------] Loss: 0.3645, Epsilon: 0.9175, Reward: 0.0\n",
      "Epoch 44/5000, [------------------------------] Loss: 0.2450, Epsilon: 0.9157, Reward: 0.0\n",
      "Epoch 45/5000, [------------------------------] Loss: 0.7899, Epsilon: 0.9138, Reward: 0.0\n",
      "Epoch 46/5000, [------------------------------] Loss: 0.4123, Epsilon: 0.9120, Reward: 0.0\n",
      "Epoch 47/5000, [------------------------------] Loss: 0.4309, Epsilon: 0.9102, Reward: 0.0\n",
      "Epoch 48/5000, [------------------------------] Loss: 0.2889, Epsilon: 0.9084, Reward: 0.0\n",
      "Epoch 49/5000, [------------------------------] Loss: 0.3687, Epsilon: 0.9066, Reward: 0.0\n",
      "Epoch 50/5000, [------------------------------] Loss: 0.3545, Epsilon: 0.9047, Reward: 0.0\n",
      "Epoch 51/5000, [------------------------------] Loss: 0.2742, Epsilon: 0.9029, Reward: 0.0\n",
      "Epoch 52/5000, [------------------------------] Loss: 0.4937, Epsilon: 0.9011, Reward: 0.0\n",
      "Epoch 53/5000, [------------------------------] Loss: 0.3570, Epsilon: 0.8993, Reward: 0.0\n",
      "Epoch 54/5000, [------------------------------] Loss: 0.0944, Epsilon: 0.8975, Reward: 800.0\n",
      "Epoch 55/5000, [------------------------------] Loss: 0.3162, Epsilon: 0.8957, Reward: 0.0\n",
      "Epoch 56/5000, [------------------------------] Loss: 0.5101, Epsilon: 0.8939, Reward: 3200.0\n",
      "Epoch 57/5000, [------------------------------] Loss: 0.2654, Epsilon: 0.8922, Reward: 1600.0\n",
      "Epoch 58/5000, [------------------------------] Loss: 0.3051, Epsilon: 0.8904, Reward: 1600.0\n",
      "Epoch 59/5000, [------------------------------] Loss: 0.7677, Epsilon: 0.8886, Reward: 800.0\n",
      "Epoch 60/5000, [------------------------------] Loss: 0.7293, Epsilon: 0.8868, Reward: 1600.0\n",
      "Epoch 61/5000, [------------------------------] Loss: 19953.0020, Epsilon: 0.8850, Reward: 800.0\n",
      "Epoch 62/5000, [------------------------------] Loss: 1.1634, Epsilon: 0.8833, Reward: 0.0\n",
      "Epoch 63/5000, [------------------------------] Loss: 1.6081, Epsilon: 0.8815, Reward: 0.0\n",
      "Epoch 64/5000, [------------------------------] Loss: 1.2574, Epsilon: 0.8797, Reward: 2400.0\n",
      "Epoch 65/5000, [------------------------------] Loss: 1.6447, Epsilon: 0.8780, Reward: 800.0\n",
      "Epoch 66/5000, [------------------------------] Loss: 2.7797, Epsilon: 0.8762, Reward: 0.0\n",
      "Epoch 67/5000, [------------------------------] Loss: 2.5203, Epsilon: 0.8745, Reward: 0.0\n",
      "Epoch 68/5000, [------------------------------] Loss: 3.9938, Epsilon: 0.8727, Reward: 0.0\n",
      "Epoch 69/5000, [------------------------------] Loss: 4.2433, Epsilon: 0.8710, Reward: 1600.0\n",
      "Epoch 70/5000, [------------------------------] Loss: 2.3527, Epsilon: 0.8692, Reward: 0.0\n",
      "Epoch 71/5000, [------------------------------] Loss: 2.4284, Epsilon: 0.8675, Reward: 0.0\n",
      "Epoch 72/5000, [------------------------------] Loss: 1.7378, Epsilon: 0.8658, Reward: 0.0\n",
      "Epoch 73/5000, [------------------------------] Loss: 3.3469, Epsilon: 0.8640, Reward: 0.0\n",
      "Epoch 74/5000, [------------------------------] Loss: 2.4095, Epsilon: 0.8623, Reward: 0.0\n",
      "Epoch 75/5000, [------------------------------] Loss: 1.2582, Epsilon: 0.8606, Reward: 0.0\n",
      "Epoch 76/5000, [------------------------------] Loss: 1.3406, Epsilon: 0.8589, Reward: 0.0\n",
      "Epoch 77/5000, [------------------------------] Loss: 0.6129, Epsilon: 0.8571, Reward: 0.0\n",
      "Epoch 78/5000, [------------------------------] Loss: 0.1560, Epsilon: 0.8554, Reward: 0.0\n",
      "Epoch 79/5000, [------------------------------] Loss: 0.1452, Epsilon: 0.8537, Reward: 0.0\n",
      "Epoch 80/5000, [------------------------------] Loss: 0.1768, Epsilon: 0.8520, Reward: 0.0\n",
      "Epoch 81/5000, [------------------------------] Loss: 0.1406, Epsilon: 0.8503, Reward: 1600.0\n",
      "Epoch 82/5000, [------------------------------] Loss: 0.1093, Epsilon: 0.8486, Reward: 0.0\n",
      "Epoch 83/5000, [------------------------------] Loss: 0.0877, Epsilon: 0.8469, Reward: 0.0\n",
      "Epoch 84/5000, [------------------------------] Loss: 0.2194, Epsilon: 0.8452, Reward: 0.0\n",
      "Epoch 85/5000, [------------------------------] Loss: 0.0504, Epsilon: 0.8435, Reward: 0.0\n",
      "Epoch 86/5000, [------------------------------] Loss: 0.0609, Epsilon: 0.8418, Reward: 1600.0\n",
      "Epoch 87/5000, [------------------------------] Loss: 0.2991, Epsilon: 0.8402, Reward: 0.0\n",
      "Epoch 88/5000, [------------------------------] Loss: 0.2677, Epsilon: 0.8385, Reward: 1600.0\n",
      "Epoch 89/5000, [------------------------------] Loss: 0.4227, Epsilon: 0.8368, Reward: 0.0\n",
      "Epoch 90/5000, [------------------------------] Loss: 0.2885, Epsilon: 0.8351, Reward: 1600.0\n",
      "Epoch 91/5000, [------------------------------] Loss: 0.3070, Epsilon: 0.8334, Reward: 0.0\n",
      "Epoch 92/5000, [------------------------------] Loss: 0.6707, Epsilon: 0.8318, Reward: 0.0\n",
      "Epoch 93/5000, [------------------------------] Loss: 0.7943, Epsilon: 0.8301, Reward: 0.0\n",
      "Epoch 94/5000, [------------------------------] Loss: 0.4012, Epsilon: 0.8285, Reward: 0.0\n",
      "Epoch 95/5000, [------------------------------] Loss: 1.3503, Epsilon: 0.8268, Reward: 0.0\n",
      "Epoch 96/5000, [------------------------------] Loss: 1.0912, Epsilon: 0.8251, Reward: 0.0\n",
      "Epoch 97/5000, [------------------------------] Loss: 1.1997, Epsilon: 0.8235, Reward: 0.0\n",
      "Epoch 98/5000, [------------------------------] Loss: 0.9558, Epsilon: 0.8219, Reward: 800.0\n",
      "Epoch 99/5000, [------------------------------] Loss: 1.3269, Epsilon: 0.8202, Reward: 1600.0\n",
      "Epoch 100/5000, [------------------------------] Loss: 0.9177, Epsilon: 0.8186, Reward: 0.0\n",
      "Epoch 101/5000, [------------------------------] Loss: 0.1023, Epsilon: 0.8169, Reward: 0.0\n",
      "Epoch 102/5000, [------------------------------] Loss: 0.0523, Epsilon: 0.8153, Reward: 800.0\n",
      "Epoch 103/5000, [------------------------------] Loss: 0.1067, Epsilon: 0.8137, Reward: 1600.0\n",
      "Epoch 104/5000, [------------------------------] Loss: 0.1353, Epsilon: 0.8120, Reward: 0.0\n",
      "Epoch 105/5000, [------------------------------] Loss: 0.1832, Epsilon: 0.8104, Reward: 0.0\n",
      "Epoch 106/5000, [------------------------------] Loss: 0.1748, Epsilon: 0.8088, Reward: 0.0\n",
      "Epoch 107/5000, [------------------------------] Loss: 0.4900, Epsilon: 0.8072, Reward: 0.0\n",
      "Epoch 108/5000, [------------------------------] Loss: 0.6167, Epsilon: 0.8056, Reward: 0.0\n",
      "Epoch 109/5000, [------------------------------] Loss: 0.3049, Epsilon: 0.8039, Reward: 3200.0\n",
      "Epoch 110/5000, [------------------------------] Loss: 0.5800, Epsilon: 0.8023, Reward: 1600.0\n",
      "Epoch 111/5000, [------------------------------] Loss: 0.7379, Epsilon: 0.8007, Reward: 0.0\n",
      "Epoch 112/5000, [------------------------------] Loss: 19914.7852, Epsilon: 0.7991, Reward: 0.0\n",
      "Epoch 113/5000, [------------------------------] Loss: 0.6838, Epsilon: 0.7975, Reward: 1600.0\n",
      "Epoch 114/5000, [------------------------------] Loss: 1.1376, Epsilon: 0.7959, Reward: 0.0\n",
      "Epoch 115/5000, [------------------------------] Loss: 1.8137, Epsilon: 0.7944, Reward: 0.0\n",
      "Epoch 116/5000, [------------------------------] Loss: 0.6554, Epsilon: 0.7928, Reward: 0.0\n",
      "Epoch 117/5000, [------------------------------] Loss: 1.5365, Epsilon: 0.7912, Reward: 0.0\n",
      "Epoch 118/5000, [------------------------------] Loss: 0.8659, Epsilon: 0.7896, Reward: 0.0\n",
      "Epoch 119/5000, [------------------------------] Loss: 1.7351, Epsilon: 0.7880, Reward: 0.0\n",
      "Epoch 120/5000, [------------------------------] Loss: 19885.3066, Epsilon: 0.7864, Reward: 1600.0\n",
      "Epoch 121/5000, [------------------------------] Loss: 1.6187, Epsilon: 0.7849, Reward: 800.0\n",
      "Epoch 122/5000, [------------------------------] Loss: 1.2078, Epsilon: 0.7833, Reward: 1600.0\n",
      "Epoch 123/5000, [------------------------------] Loss: 1.0833, Epsilon: 0.7817, Reward: 1600.0\n",
      "Epoch 124/5000, [------------------------------] Loss: 1.3388, Epsilon: 0.7802, Reward: 0.0\n",
      "Epoch 125/5000, [------------------------------] Loss: 0.9312, Epsilon: 0.7786, Reward: 0.0\n",
      "Epoch 126/5000, [------------------------------] Loss: 1.1126, Epsilon: 0.7770, Reward: 0.0\n",
      "Epoch 127/5000, [------------------------------] Loss: 0.7362, Epsilon: 0.7755, Reward: 0.0\n",
      "Epoch 128/5000, [------------------------------] Loss: 1.0856, Epsilon: 0.7739, Reward: 0.0\n",
      "Epoch 129/5000, [------------------------------] Loss: 19920.6875, Epsilon: 0.7724, Reward: 0.0\n",
      "Epoch 130/5000, [------------------------------] Loss: 0.7547, Epsilon: 0.7709, Reward: 0.0\n",
      "Epoch 131/5000, [------------------------------] Loss: 0.7093, Epsilon: 0.7693, Reward: 0.0\n",
      "Epoch 132/5000, [------------------------------] Loss: 1.1432, Epsilon: 0.7678, Reward: 0.0\n",
      "Epoch 133/5000, [------------------------------] Loss: 0.6616, Epsilon: 0.7662, Reward: 0.0\n",
      "Epoch 134/5000, [------------------------------] Loss: 0.5927, Epsilon: 0.7647, Reward: 0.0\n",
      "Epoch 135/5000, [------------------------------] Loss: 0.7887, Epsilon: 0.7632, Reward: 0.0\n",
      "Epoch 136/5000, [------------------------------] Loss: 0.6530, Epsilon: 0.7616, Reward: 1600.0\n",
      "Epoch 137/5000, [------------------------------] Loss: 0.5634, Epsilon: 0.7601, Reward: 1600.0\n",
      "Epoch 138/5000, [------------------------------] Loss: 0.8303, Epsilon: 0.7586, Reward: 0.0\n",
      "Epoch 139/5000, [------------------------------] Loss: 0.3782, Epsilon: 0.7571, Reward: 0.0\n",
      "Epoch 140/5000, [------------------------------] Loss: 0.7141, Epsilon: 0.7556, Reward: 0.0\n",
      "Epoch 141/5000, [------------------------------] Loss: 0.5878, Epsilon: 0.7541, Reward: 0.0\n",
      "Epoch 142/5000, [------------------------------] Loss: 0.5591, Epsilon: 0.7526, Reward: 0.0\n",
      "Epoch 143/5000, [------------------------------] Loss: 0.2607, Epsilon: 0.7510, Reward: 0.0\n",
      "Epoch 144/5000, [------------------------------] Loss: 0.3580, Epsilon: 0.7495, Reward: 0.0\n",
      "Epoch 145/5000, [------------------------------] Loss: 0.1233, Epsilon: 0.7480, Reward: 0.0\n",
      "Epoch 146/5000, [------------------------------] Loss: 0.3188, Epsilon: 0.7466, Reward: 1600.0\n",
      "Epoch 147/5000, [------------------------------] Loss: 0.1638, Epsilon: 0.7451, Reward: 0.0\n",
      "Epoch 148/5000, [------------------------------] Loss: 0.3600, Epsilon: 0.7436, Reward: 0.0\n",
      "Epoch 149/5000, [------------------------------] Loss: 0.6121, Epsilon: 0.7421, Reward: 0.0\n",
      "Epoch 150/5000, [------------------------------] Loss: 0.4735, Epsilon: 0.7406, Reward: 0.0\n",
      "Epoch 151/5000, [------------------------------] Loss: 0.5559, Epsilon: 0.7391, Reward: 0.0\n",
      "Epoch 152/5000, [------------------------------] Loss: 0.4359, Epsilon: 0.7376, Reward: 0.0\n",
      "Epoch 153/5000, [------------------------------] Loss: 0.3742, Epsilon: 0.7362, Reward: 0.0\n",
      "Epoch 154/5000, [------------------------------] Loss: 0.3277, Epsilon: 0.7347, Reward: 0.0\n",
      "Epoch 155/5000, [------------------------------] Loss: 0.1148, Epsilon: 0.7332, Reward: 1600.0\n",
      "Epoch 156/5000, [------------------------------] Loss: 0.2096, Epsilon: 0.7318, Reward: 0.0\n",
      "Epoch 157/5000, [------------------------------] Loss: 0.3156, Epsilon: 0.7303, Reward: 0.0\n",
      "Epoch 158/5000, [------------------------------] Loss: 0.1434, Epsilon: 0.7288, Reward: 0.0\n",
      "Epoch 159/5000, [------------------------------] Loss: 0.6157, Epsilon: 0.7274, Reward: 0.0\n",
      "Epoch 160/5000, [------------------------------] Loss: 0.4355, Epsilon: 0.7259, Reward: 800.0\n",
      "Epoch 161/5000, [------------------------------] Loss: 0.1541, Epsilon: 0.7245, Reward: 0.0\n",
      "Epoch 162/5000, [------------------------------] Loss: 0.1535, Epsilon: 0.7230, Reward: 0.0\n",
      "Epoch 163/5000, [------------------------------] Loss: 0.3078, Epsilon: 0.7216, Reward: 0.0\n",
      "Epoch 164/5000, [------------------------------] Loss: 0.0977, Epsilon: 0.7201, Reward: 0.0\n",
      "Epoch 165/5000, [------------------------------] Loss: 0.0678, Epsilon: 0.7187, Reward: 0.0\n",
      "Epoch 166/5000, [------------------------------] Loss: 0.0883, Epsilon: 0.7172, Reward: 0.0\n",
      "Epoch 167/5000, [#-----------------------------] Loss: 0.2065, Epsilon: 0.7158, Reward: 0.0\n",
      "Epoch 168/5000, [#-----------------------------] Loss: 0.1191, Epsilon: 0.7144, Reward: 1600.0\n",
      "Epoch 169/5000, [#-----------------------------] Loss: 0.2323, Epsilon: 0.7130, Reward: 0.0\n",
      "Epoch 170/5000, [#-----------------------------] Loss: 0.2274, Epsilon: 0.7115, Reward: 0.0\n",
      "Epoch 171/5000, [#-----------------------------] Loss: 0.0578, Epsilon: 0.7101, Reward: 0.0\n",
      "Epoch 172/5000, [#-----------------------------] Loss: 0.0505, Epsilon: 0.7087, Reward: 0.0\n",
      "Epoch 173/5000, [#-----------------------------] Loss: 0.0661, Epsilon: 0.7073, Reward: 0.0\n",
      "Epoch 174/5000, [#-----------------------------] Loss: 0.1358, Epsilon: 0.7059, Reward: 1600.0\n",
      "Epoch 175/5000, [#-----------------------------] Loss: 0.1468, Epsilon: 0.7044, Reward: 0.0\n",
      "Epoch 176/5000, [#-----------------------------] Loss: 0.1596, Epsilon: 0.7030, Reward: 0.0\n",
      "Epoch 177/5000, [#-----------------------------] Loss: 0.1317, Epsilon: 0.7016, Reward: 800.0\n",
      "Epoch 178/5000, [#-----------------------------] Loss: 0.3110, Epsilon: 0.7002, Reward: 0.0\n",
      "Epoch 179/5000, [#-----------------------------] Loss: 0.4657, Epsilon: 0.6988, Reward: 800.0\n",
      "Epoch 180/5000, [#-----------------------------] Loss: 0.4095, Epsilon: 0.6974, Reward: 1600.0\n",
      "Epoch 181/5000, [#-----------------------------] Loss: 0.3322, Epsilon: 0.6960, Reward: 0.0\n",
      "Epoch 182/5000, [#-----------------------------] Loss: 1.2137, Epsilon: 0.6946, Reward: 1600.0\n",
      "Epoch 183/5000, [#-----------------------------] Loss: 0.8382, Epsilon: 0.6932, Reward: 0.0\n",
      "Epoch 184/5000, [#-----------------------------] Loss: 0.3992, Epsilon: 0.6919, Reward: 1600.0\n",
      "Epoch 185/5000, [#-----------------------------] Loss: 1.0136, Epsilon: 0.6905, Reward: 1600.0\n",
      "Epoch 186/5000, [#-----------------------------] Loss: 1.5029, Epsilon: 0.6891, Reward: 0.0\n",
      "Epoch 187/5000, [#-----------------------------] Loss: 1.2803, Epsilon: 0.6877, Reward: 0.0\n",
      "Epoch 188/5000, [#-----------------------------] Loss: 0.9151, Epsilon: 0.6863, Reward: 0.0\n",
      "Epoch 189/5000, [#-----------------------------] Loss: 2.3370, Epsilon: 0.6850, Reward: 0.0\n",
      "Epoch 190/5000, [#-----------------------------] Loss: 1.7934, Epsilon: 0.6836, Reward: 1600.0\n",
      "Epoch 191/5000, [#-----------------------------] Loss: 1.4894, Epsilon: 0.6822, Reward: 1600.0\n",
      "Epoch 192/5000, [#-----------------------------] Loss: 1.7392, Epsilon: 0.6809, Reward: 1600.0\n",
      "Epoch 193/5000, [#-----------------------------] Loss: 2.1648, Epsilon: 0.6795, Reward: 0.0\n",
      "Epoch 194/5000, [#-----------------------------] Loss: 2.9752, Epsilon: 0.6781, Reward: 1600.0\n",
      "Epoch 195/5000, [#-----------------------------] Loss: 2.1005, Epsilon: 0.6768, Reward: 0.0\n",
      "Epoch 196/5000, [#-----------------------------] Loss: 2.0445, Epsilon: 0.6754, Reward: 0.0\n",
      "Epoch 197/5000, [#-----------------------------] Loss: 1.6558, Epsilon: 0.6741, Reward: 0.0\n",
      "Epoch 198/5000, [#-----------------------------] Loss: 1.5554, Epsilon: 0.6727, Reward: 0.0\n",
      "Epoch 199/5000, [#-----------------------------] Loss: 1.8416, Epsilon: 0.6714, Reward: 0.0\n",
      "Epoch 200/5000, [#-----------------------------] Loss: 1.4659, Epsilon: 0.6701, Reward: 0.0\n",
      "Epoch 201/5000, [#-----------------------------] Loss: 0.0874, Epsilon: 0.6687, Reward: 0.0\n",
      "Epoch 202/5000, [#-----------------------------] Loss: 0.3371, Epsilon: 0.6674, Reward: 1600.0\n",
      "Epoch 203/5000, [#-----------------------------] Loss: 0.6336, Epsilon: 0.6660, Reward: 0.0\n",
      "Epoch 204/5000, [#-----------------------------] Loss: 0.4584, Epsilon: 0.6647, Reward: 0.0\n",
      "Epoch 205/5000, [#-----------------------------] Loss: 0.4141, Epsilon: 0.6634, Reward: 0.0\n",
      "Epoch 206/5000, [#-----------------------------] Loss: 0.2660, Epsilon: 0.6621, Reward: 0.0\n",
      "Epoch 207/5000, [#-----------------------------] Loss: 0.5558, Epsilon: 0.6607, Reward: 1600.0\n",
      "Epoch 208/5000, [#-----------------------------] Loss: 0.2795, Epsilon: 0.6594, Reward: 0.0\n",
      "Epoch 209/5000, [#-----------------------------] Loss: 0.2530, Epsilon: 0.6581, Reward: 0.0\n",
      "Epoch 210/5000, [#-----------------------------] Loss: 0.4666, Epsilon: 0.6568, Reward: 800.0\n",
      "Epoch 211/5000, [#-----------------------------] Loss: 0.2892, Epsilon: 0.6555, Reward: 0.0\n",
      "Epoch 212/5000, [#-----------------------------] Loss: 0.2180, Epsilon: 0.6541, Reward: 0.0\n",
      "Epoch 213/5000, [#-----------------------------] Loss: 0.3174, Epsilon: 0.6528, Reward: 1600.0\n",
      "Epoch 214/5000, [#-----------------------------] Loss: 0.2419, Epsilon: 0.6515, Reward: 0.0\n",
      "Epoch 215/5000, [#-----------------------------] Loss: 0.1239, Epsilon: 0.6502, Reward: 0.0\n",
      "Epoch 216/5000, [#-----------------------------] Loss: 0.2665, Epsilon: 0.6489, Reward: 0.0\n",
      "Epoch 217/5000, [#-----------------------------] Loss: 0.2285, Epsilon: 0.6476, Reward: 0.0\n",
      "Epoch 218/5000, [#-----------------------------] Loss: 0.3515, Epsilon: 0.6463, Reward: 0.0\n",
      "Epoch 219/5000, [#-----------------------------] Loss: 19988.0703, Epsilon: 0.6450, Reward: 0.0\n",
      "Epoch 220/5000, [#-----------------------------] Loss: 0.7309, Epsilon: 0.6438, Reward: 0.0\n",
      "Epoch 221/5000, [#-----------------------------] Loss: 0.4611, Epsilon: 0.6425, Reward: 0.0\n",
      "Epoch 222/5000, [#-----------------------------] Loss: 0.2337, Epsilon: 0.6412, Reward: 0.0\n",
      "Epoch 223/5000, [#-----------------------------] Loss: 0.2955, Epsilon: 0.6399, Reward: 0.0\n",
      "Epoch 224/5000, [#-----------------------------] Loss: 0.3174, Epsilon: 0.6386, Reward: 0.0\n",
      "Epoch 225/5000, [#-----------------------------] Loss: 0.2072, Epsilon: 0.6373, Reward: 0.0\n",
      "Epoch 226/5000, [#-----------------------------] Loss: 0.0177, Epsilon: 0.6361, Reward: 0.0\n",
      "Epoch 227/5000, [#-----------------------------] Loss: 0.0001, Epsilon: 0.6348, Reward: 0.0\n",
      "Epoch 228/5000, [#-----------------------------] Loss: 0.0000, Epsilon: 0.6335, Reward: 0.0\n",
      "Epoch 229/5000, [#-----------------------------] Loss: 0.0000, Epsilon: 0.6323, Reward: 0.0\n",
      "Epoch 230/5000, [#-----------------------------] Loss: 0.0002, Epsilon: 0.6310, Reward: 0.0\n",
      "Epoch 231/5000, [#-----------------------------] Loss: 0.0000, Epsilon: 0.6297, Reward: 0.0\n",
      "Epoch 232/5000, [#-----------------------------] Loss: 0.0000, Epsilon: 0.6285, Reward: 1600.0\n",
      "Epoch 233/5000, [#-----------------------------] Loss: 0.0020, Epsilon: 0.6272, Reward: 1600.0\n",
      "Epoch 234/5000, [#-----------------------------] Loss: 0.0401, Epsilon: 0.6260, Reward: 0.0\n",
      "Epoch 235/5000, [#-----------------------------] Loss: 0.1243, Epsilon: 0.6247, Reward: 0.0\n",
      "Epoch 236/5000, [#-----------------------------] Loss: 0.1238, Epsilon: 0.6235, Reward: 0.0\n",
      "Epoch 237/5000, [#-----------------------------] Loss: 0.1114, Epsilon: 0.6222, Reward: 800.0\n",
      "Epoch 238/5000, [#-----------------------------] Loss: 0.3811, Epsilon: 0.6210, Reward: 0.0\n",
      "Epoch 239/5000, [#-----------------------------] Loss: 0.2092, Epsilon: 0.6197, Reward: 0.0\n",
      "Epoch 240/5000, [#-----------------------------] Loss: 0.2254, Epsilon: 0.6185, Reward: 1600.0\n",
      "Epoch 241/5000, [#-----------------------------] Loss: 0.3243, Epsilon: 0.6172, Reward: 0.0\n",
      "Epoch 242/5000, [#-----------------------------] Loss: 0.3571, Epsilon: 0.6160, Reward: 0.0\n",
      "Epoch 243/5000, [#-----------------------------] Loss: 0.5249, Epsilon: 0.6148, Reward: 0.0\n",
      "Epoch 244/5000, [#-----------------------------] Loss: 0.3249, Epsilon: 0.6136, Reward: 0.0\n",
      "Epoch 245/5000, [#-----------------------------] Loss: 0.2926, Epsilon: 0.6123, Reward: 800.0\n",
      "Epoch 246/5000, [#-----------------------------] Loss: 0.4022, Epsilon: 0.6111, Reward: 1600.0\n",
      "Epoch 247/5000, [#-----------------------------] Loss: 0.4180, Epsilon: 0.6099, Reward: 0.0\n",
      "Epoch 248/5000, [#-----------------------------] Loss: 0.6603, Epsilon: 0.6087, Reward: 0.0\n",
      "Epoch 249/5000, [#-----------------------------] Loss: 0.9364, Epsilon: 0.6074, Reward: 1600.0\n",
      "Epoch 250/5000, [#-----------------------------] Loss: 1.1021, Epsilon: 0.6062, Reward: 800.0\n",
      "Epoch 251/5000, [#-----------------------------] Loss: 0.6026, Epsilon: 0.6050, Reward: 0.0\n",
      "Epoch 252/5000, [#-----------------------------] Loss: 0.9095, Epsilon: 0.6038, Reward: 800.0\n",
      "Epoch 253/5000, [#-----------------------------] Loss: 0.8324, Epsilon: 0.6026, Reward: 0.0\n",
      "Epoch 254/5000, [#-----------------------------] Loss: 0.8514, Epsilon: 0.6014, Reward: 0.0\n",
      "Epoch 255/5000, [#-----------------------------] Loss: 1.0587, Epsilon: 0.6002, Reward: 0.0\n",
      "Epoch 256/5000, [#-----------------------------] Loss: 0.9439, Epsilon: 0.5990, Reward: 0.0\n",
      "Epoch 257/5000, [#-----------------------------] Loss: 0.3832, Epsilon: 0.5978, Reward: 0.0\n",
      "Epoch 258/5000, [#-----------------------------] Loss: 0.4314, Epsilon: 0.5966, Reward: 0.0\n",
      "Epoch 259/5000, [#-----------------------------] Loss: 0.4741, Epsilon: 0.5954, Reward: 800.0\n",
      "Epoch 260/5000, [#-----------------------------] Loss: 0.9510, Epsilon: 0.5942, Reward: 0.0\n",
      "Epoch 261/5000, [#-----------------------------] Loss: 0.7381, Epsilon: 0.5930, Reward: 0.0\n",
      "Epoch 262/5000, [#-----------------------------] Loss: 0.4013, Epsilon: 0.5918, Reward: 1600.0\n",
      "Epoch 263/5000, [#-----------------------------] Loss: 0.4811, Epsilon: 0.5907, Reward: 0.0\n",
      "Epoch 264/5000, [#-----------------------------] Loss: 0.7072, Epsilon: 0.5895, Reward: 0.0\n",
      "Epoch 265/5000, [#-----------------------------] Loss: 0.5005, Epsilon: 0.5883, Reward: 0.0\n",
      "Epoch 266/5000, [#-----------------------------] Loss: 0.5220, Epsilon: 0.5871, Reward: 0.0\n",
      "Epoch 267/5000, [#-----------------------------] Loss: 0.4143, Epsilon: 0.5859, Reward: 0.0\n",
      "Epoch 268/5000, [#-----------------------------] Loss: 0.2247, Epsilon: 0.5848, Reward: 0.0\n",
      "Epoch 269/5000, [#-----------------------------] Loss: 0.5289, Epsilon: 0.5836, Reward: 0.0\n",
      "Epoch 270/5000, [#-----------------------------] Loss: 0.3567, Epsilon: 0.5824, Reward: 0.0\n",
      "Epoch 271/5000, [#-----------------------------] Loss: 0.3243, Epsilon: 0.5813, Reward: 0.0\n",
      "Epoch 272/5000, [#-----------------------------] Loss: 0.1799, Epsilon: 0.5801, Reward: 0.0\n",
      "Epoch 273/5000, [#-----------------------------] Loss: 0.1790, Epsilon: 0.5789, Reward: 0.0\n",
      "Epoch 274/5000, [#-----------------------------] Loss: 0.0724, Epsilon: 0.5778, Reward: 1600.0\n",
      "Epoch 275/5000, [#-----------------------------] Loss: 0.3772, Epsilon: 0.5766, Reward: 1600.0\n",
      "Epoch 276/5000, [#-----------------------------] Loss: 0.1130, Epsilon: 0.5755, Reward: 800.0\n",
      "Epoch 277/5000, [#-----------------------------] Loss: 0.5307, Epsilon: 0.5743, Reward: 0.0\n",
      "Epoch 278/5000, [#-----------------------------] Loss: 0.3938, Epsilon: 0.5732, Reward: 0.0\n",
      "Epoch 279/5000, [#-----------------------------] Loss: 0.9567, Epsilon: 0.5720, Reward: 0.0\n",
      "Epoch 280/5000, [#-----------------------------] Loss: 0.5214, Epsilon: 0.5709, Reward: 0.0\n",
      "Epoch 281/5000, [#-----------------------------] Loss: 0.7377, Epsilon: 0.5697, Reward: 0.0\n",
      "Epoch 282/5000, [#-----------------------------] Loss: 0.7719, Epsilon: 0.5686, Reward: 0.0\n",
      "Epoch 283/5000, [#-----------------------------] Loss: 0.8357, Epsilon: 0.5675, Reward: 0.0\n",
      "Epoch 284/5000, [#-----------------------------] Loss: 0.4791, Epsilon: 0.5663, Reward: 0.0\n",
      "Epoch 285/5000, [#-----------------------------] Loss: 0.2857, Epsilon: 0.5652, Reward: 0.0\n",
      "Epoch 286/5000, [#-----------------------------] Loss: 0.4467, Epsilon: 0.5641, Reward: 0.0\n",
      "Epoch 287/5000, [#-----------------------------] Loss: 0.3628, Epsilon: 0.5629, Reward: 800.0\n",
      "Epoch 288/5000, [#-----------------------------] Loss: 0.7137, Epsilon: 0.5618, Reward: 0.0\n",
      "Epoch 289/5000, [#-----------------------------] Loss: 0.6150, Epsilon: 0.5607, Reward: 0.0\n",
      "Epoch 290/5000, [#-----------------------------] Loss: 0.6114, Epsilon: 0.5596, Reward: 0.0\n",
      "Epoch 291/5000, [#-----------------------------] Loss: 0.4770, Epsilon: 0.5585, Reward: 0.0\n",
      "Epoch 292/5000, [#-----------------------------] Loss: 0.3971, Epsilon: 0.5573, Reward: 0.0\n",
      "Epoch 293/5000, [#-----------------------------] Loss: 0.3375, Epsilon: 0.5562, Reward: 1600.0\n",
      "Epoch 294/5000, [#-----------------------------] Loss: 0.1693, Epsilon: 0.5551, Reward: 800.0\n",
      "Epoch 295/5000, [#-----------------------------] Loss: 0.6223, Epsilon: 0.5540, Reward: 0.0\n",
      "Epoch 296/5000, [#-----------------------------] Loss: 0.2451, Epsilon: 0.5529, Reward: 0.0\n",
      "Epoch 297/5000, [#-----------------------------] Loss: 0.1784, Epsilon: 0.5518, Reward: 0.0\n",
      "Epoch 298/5000, [#-----------------------------] Loss: 0.1765, Epsilon: 0.5507, Reward: 0.0\n",
      "Epoch 299/5000, [#-----------------------------] Loss: 0.2869, Epsilon: 0.5496, Reward: 0.0\n",
      "Epoch 300/5000, [#-----------------------------] Loss: 0.1915, Epsilon: 0.5485, Reward: 0.0\n",
      "Epoch 301/5000, [#-----------------------------] Loss: 0.0421, Epsilon: 0.5474, Reward: 0.0\n",
      "Epoch 302/5000, [#-----------------------------] Loss: 0.0676, Epsilon: 0.5463, Reward: 0.0\n",
      "Epoch 303/5000, [#-----------------------------] Loss: 0.1015, Epsilon: 0.5452, Reward: 1600.0\n",
      "Epoch 304/5000, [#-----------------------------] Loss: 0.1406, Epsilon: 0.5441, Reward: 800.0\n",
      "Epoch 305/5000, [#-----------------------------] Loss: 0.2113, Epsilon: 0.5430, Reward: 1600.0\n",
      "Epoch 306/5000, [#-----------------------------] Loss: 0.2995, Epsilon: 0.5419, Reward: 0.0\n",
      "Epoch 307/5000, [#-----------------------------] Loss: 1.0921, Epsilon: 0.5408, Reward: 0.0\n",
      "Epoch 308/5000, [#-----------------------------] Loss: 0.5405, Epsilon: 0.5398, Reward: 0.0\n",
      "Epoch 309/5000, [#-----------------------------] Loss: 0.6890, Epsilon: 0.5387, Reward: 0.0\n",
      "Epoch 310/5000, [#-----------------------------] Loss: 0.6979, Epsilon: 0.5376, Reward: 0.0\n",
      "Epoch 311/5000, [#-----------------------------] Loss: 1.4139, Epsilon: 0.5365, Reward: 800.0\n",
      "Epoch 312/5000, [#-----------------------------] Loss: 0.3457, Epsilon: 0.5355, Reward: 0.0\n",
      "Epoch 313/5000, [#-----------------------------] Loss: 0.1432, Epsilon: 0.5344, Reward: 0.0\n",
      "Epoch 314/5000, [#-----------------------------] Loss: 0.4994, Epsilon: 0.5333, Reward: 0.0\n",
      "Epoch 315/5000, [#-----------------------------] Loss: 0.2921, Epsilon: 0.5323, Reward: 0.0\n",
      "Epoch 316/5000, [#-----------------------------] Loss: 0.3809, Epsilon: 0.5312, Reward: 0.0\n",
      "Epoch 317/5000, [#-----------------------------] Loss: 0.2938, Epsilon: 0.5301, Reward: 1600.0\n",
      "Epoch 318/5000, [#-----------------------------] Loss: 0.2207, Epsilon: 0.5291, Reward: 1600.0\n",
      "Epoch 319/5000, [#-----------------------------] Loss: 0.2400, Epsilon: 0.5280, Reward: 0.0\n",
      "Epoch 320/5000, [#-----------------------------] Loss: 0.3746, Epsilon: 0.5270, Reward: 1600.0\n",
      "Epoch 321/5000, [#-----------------------------] Loss: 0.6025, Epsilon: 0.5259, Reward: 800.0\n",
      "Epoch 322/5000, [#-----------------------------] Loss: 0.5594, Epsilon: 0.5248, Reward: 0.0\n",
      "Epoch 323/5000, [#-----------------------------] Loss: 0.5512, Epsilon: 0.5238, Reward: 0.0\n",
      "Epoch 324/5000, [#-----------------------------] Loss: 0.7932, Epsilon: 0.5228, Reward: 0.0\n",
      "Epoch 325/5000, [#-----------------------------] Loss: 0.4746, Epsilon: 0.5217, Reward: 0.0\n",
      "Epoch 326/5000, [#-----------------------------] Loss: 0.6509, Epsilon: 0.5207, Reward: 0.0\n",
      "Epoch 327/5000, [#-----------------------------] Loss: 0.6752, Epsilon: 0.5196, Reward: 0.0\n",
      "Epoch 328/5000, [#-----------------------------] Loss: 0.5580, Epsilon: 0.5186, Reward: 0.0\n",
      "Epoch 329/5000, [#-----------------------------] Loss: 0.7717, Epsilon: 0.5175, Reward: 1600.0\n",
      "Epoch 330/5000, [#-----------------------------] Loss: 0.8083, Epsilon: 0.5165, Reward: 800.0\n",
      "Epoch 331/5000, [#-----------------------------] Loss: 1.3514, Epsilon: 0.5155, Reward: 0.0\n",
      "Epoch 332/5000, [#-----------------------------] Loss: 1.0084, Epsilon: 0.5144, Reward: 0.0\n",
      "Epoch 333/5000, [#-----------------------------] Loss: 0.9388, Epsilon: 0.5134, Reward: 800.0\n",
      "Epoch 334/5000, [##----------------------------] Loss: 0.6318, Epsilon: 0.5124, Reward: 0.0\n",
      "Epoch 335/5000, [##----------------------------] Loss: 0.6579, Epsilon: 0.5114, Reward: 0.0\n",
      "Epoch 336/5000, [##----------------------------] Loss: 0.4694, Epsilon: 0.5103, Reward: 0.0\n",
      "Epoch 337/5000, [##----------------------------] Loss: 0.7116, Epsilon: 0.5093, Reward: 1600.0\n",
      "Epoch 338/5000, [##----------------------------] Loss: 0.4605, Epsilon: 0.5083, Reward: 0.0\n",
      "Epoch 339/5000, [##----------------------------] Loss: 0.5527, Epsilon: 0.5073, Reward: 1600.0\n",
      "Epoch 340/5000, [##----------------------------] Loss: 0.3034, Epsilon: 0.5063, Reward: 0.0\n",
      "Epoch 341/5000, [##----------------------------] Loss: 0.8245, Epsilon: 0.5053, Reward: 1600.0\n",
      "Epoch 342/5000, [##----------------------------] Loss: 1.3387, Epsilon: 0.5042, Reward: 0.0\n",
      "Epoch 343/5000, [##----------------------------] Loss: 1.7454, Epsilon: 0.5032, Reward: 0.0\n",
      "Epoch 344/5000, [##----------------------------] Loss: 1.1837, Epsilon: 0.5022, Reward: 0.0\n",
      "Epoch 345/5000, [##----------------------------] Loss: 1.5047, Epsilon: 0.5012, Reward: 0.0\n",
      "Epoch 346/5000, [##----------------------------] Loss: 1.1551, Epsilon: 0.5002, Reward: 1600.0\n",
      "Epoch 347/5000, [##----------------------------] Loss: 1.0059, Epsilon: 0.4992, Reward: 0.0\n",
      "Epoch 348/5000, [##----------------------------] Loss: 0.7205, Epsilon: 0.4982, Reward: 0.0\n",
      "Epoch 349/5000, [##----------------------------] Loss: 0.5738, Epsilon: 0.4972, Reward: 0.0\n",
      "Epoch 350/5000, [##----------------------------] Loss: 0.8631, Epsilon: 0.4962, Reward: 1600.0\n",
      "Epoch 351/5000, [##----------------------------] Loss: 19900.2734, Epsilon: 0.4952, Reward: 0.0\n",
      "Epoch 352/5000, [##----------------------------] Loss: 19942.4805, Epsilon: 0.4943, Reward: 1600.0\n",
      "Epoch 353/5000, [##----------------------------] Loss: 1.6167, Epsilon: 0.4933, Reward: 0.0\n",
      "Epoch 354/5000, [##----------------------------] Loss: 1.2575, Epsilon: 0.4923, Reward: 800.0\n",
      "Epoch 355/5000, [##----------------------------] Loss: 1.5319, Epsilon: 0.4913, Reward: 0.0\n",
      "Epoch 356/5000, [##----------------------------] Loss: 1.4487, Epsilon: 0.4903, Reward: 0.0\n",
      "Epoch 357/5000, [##----------------------------] Loss: 1.6347, Epsilon: 0.4893, Reward: 0.0\n",
      "Epoch 358/5000, [##----------------------------] Loss: 0.9288, Epsilon: 0.4884, Reward: 0.0\n",
      "Epoch 359/5000, [##----------------------------] Loss: 0.4474, Epsilon: 0.4874, Reward: 0.0\n",
      "Epoch 360/5000, [##----------------------------] Loss: 0.3869, Epsilon: 0.4864, Reward: 0.0\n",
      "Epoch 361/5000, [##----------------------------] Loss: 0.5024, Epsilon: 0.4854, Reward: 0.0\n",
      "Epoch 362/5000, [##----------------------------] Loss: 0.3891, Epsilon: 0.4845, Reward: 2400.0\n",
      "Epoch 363/5000, [##----------------------------] Loss: 0.3041, Epsilon: 0.4835, Reward: 0.0\n",
      "Epoch 364/5000, [##----------------------------] Loss: 0.3886, Epsilon: 0.4825, Reward: 0.0\n",
      "Epoch 365/5000, [##----------------------------] Loss: 0.2518, Epsilon: 0.4816, Reward: 0.0\n",
      "Epoch 366/5000, [##----------------------------] Loss: 0.1807, Epsilon: 0.4806, Reward: 0.0\n",
      "Epoch 367/5000, [##----------------------------] Loss: 0.3478, Epsilon: 0.4796, Reward: 0.0\n",
      "Epoch 368/5000, [##----------------------------] Loss: 0.1966, Epsilon: 0.4787, Reward: 0.0\n",
      "Epoch 369/5000, [##----------------------------] Loss: 0.3550, Epsilon: 0.4777, Reward: 800.0\n",
      "Epoch 370/5000, [##----------------------------] Loss: 0.2384, Epsilon: 0.4768, Reward: 0.0\n",
      "Epoch 371/5000, [##----------------------------] Loss: 0.2451, Epsilon: 0.4758, Reward: 0.0\n",
      "Epoch 372/5000, [##----------------------------] Loss: 0.2416, Epsilon: 0.4749, Reward: 0.0\n",
      "Epoch 373/5000, [##----------------------------] Loss: 0.1247, Epsilon: 0.4739, Reward: 0.0\n",
      "Epoch 374/5000, [##----------------------------] Loss: 0.0854, Epsilon: 0.4730, Reward: 0.0\n",
      "Epoch 375/5000, [##----------------------------] Loss: 0.1051, Epsilon: 0.4720, Reward: 0.0\n",
      "Epoch 376/5000, [##----------------------------] Loss: 0.0289, Epsilon: 0.4711, Reward: 0.0\n",
      "Epoch 377/5000, [##----------------------------] Loss: 0.0139, Epsilon: 0.4701, Reward: 0.0\n",
      "Epoch 378/5000, [##----------------------------] Loss: 0.0884, Epsilon: 0.4692, Reward: 800.0\n",
      "Epoch 379/5000, [##----------------------------] Loss: 0.0400, Epsilon: 0.4682, Reward: 0.0\n",
      "Epoch 380/5000, [##----------------------------] Loss: 0.0140, Epsilon: 0.4673, Reward: 0.0\n",
      "Epoch 381/5000, [##----------------------------] Loss: 0.0405, Epsilon: 0.4664, Reward: 0.0\n",
      "Epoch 382/5000, [##----------------------------] Loss: 0.0439, Epsilon: 0.4654, Reward: 0.0\n",
      "Epoch 383/5000, [##----------------------------] Loss: 0.0029, Epsilon: 0.4645, Reward: 1600.0\n",
      "Epoch 384/5000, [##----------------------------] Loss: 0.0639, Epsilon: 0.4636, Reward: 0.0\n",
      "Epoch 385/5000, [##----------------------------] Loss: 0.0241, Epsilon: 0.4627, Reward: 0.0\n",
      "Epoch 386/5000, [##----------------------------] Loss: 0.3286, Epsilon: 0.4617, Reward: 800.0\n",
      "Epoch 387/5000, [##----------------------------] Loss: 0.0991, Epsilon: 0.4608, Reward: 0.0\n",
      "Epoch 388/5000, [##----------------------------] Loss: 0.1489, Epsilon: 0.4599, Reward: 0.0\n",
      "Epoch 389/5000, [##----------------------------] Loss: 0.1575, Epsilon: 0.4590, Reward: 0.0\n",
      "Epoch 390/5000, [##----------------------------] Loss: 0.1364, Epsilon: 0.4580, Reward: 0.0\n",
      "Epoch 391/5000, [##----------------------------] Loss: 0.1262, Epsilon: 0.4571, Reward: 0.0\n",
      "Epoch 392/5000, [##----------------------------] Loss: 0.1793, Epsilon: 0.4562, Reward: 0.0\n",
      "Epoch 393/5000, [##----------------------------] Loss: 0.1285, Epsilon: 0.4553, Reward: 0.0\n",
      "Epoch 394/5000, [##----------------------------] Loss: 0.1901, Epsilon: 0.4544, Reward: 0.0\n",
      "Epoch 395/5000, [##----------------------------] Loss: 0.1673, Epsilon: 0.4535, Reward: 0.0\n",
      "Epoch 396/5000, [##----------------------------] Loss: 0.0917, Epsilon: 0.4526, Reward: 0.0\n",
      "Epoch 397/5000, [##----------------------------] Loss: 0.0501, Epsilon: 0.4517, Reward: 0.0\n",
      "Epoch 398/5000, [##----------------------------] Loss: 0.0066, Epsilon: 0.4508, Reward: 0.0\n",
      "Epoch 399/5000, [##----------------------------] Loss: 0.0001, Epsilon: 0.4499, Reward: 0.0\n",
      "Epoch 400/5000, [##----------------------------] Loss: 0.0008, Epsilon: 0.4490, Reward: 0.0\n",
      "Epoch 401/5000, [##----------------------------] Loss: 0.0002, Epsilon: 0.4481, Reward: 0.0\n",
      "Epoch 402/5000, [##----------------------------] Loss: 0.0006, Epsilon: 0.4472, Reward: 0.0\n",
      "Epoch 403/5000, [##----------------------------] Loss: 0.0005, Epsilon: 0.4463, Reward: 0.0\n",
      "Epoch 404/5000, [##----------------------------] Loss: 0.0001, Epsilon: 0.4454, Reward: 0.0\n",
      "Epoch 405/5000, [##----------------------------] Loss: 0.0341, Epsilon: 0.4445, Reward: 1600.0\n",
      "Epoch 406/5000, [##----------------------------] Loss: 0.0906, Epsilon: 0.4436, Reward: 800.0\n",
      "Epoch 407/5000, [##----------------------------] Loss: 0.0469, Epsilon: 0.4427, Reward: 0.0\n",
      "Epoch 408/5000, [##----------------------------] Loss: 0.0351, Epsilon: 0.4418, Reward: 0.0\n",
      "Epoch 409/5000, [##----------------------------] Loss: 0.1493, Epsilon: 0.4410, Reward: 3200.0\n",
      "Epoch 410/5000, [##----------------------------] Loss: 0.1662, Epsilon: 0.4401, Reward: 0.0\n",
      "Epoch 411/5000, [##----------------------------] Loss: 0.6668, Epsilon: 0.4392, Reward: 800.0\n",
      "Epoch 412/5000, [##----------------------------] Loss: 0.3217, Epsilon: 0.4383, Reward: 0.0\n",
      "Epoch 413/5000, [##----------------------------] Loss: 1.1514, Epsilon: 0.4374, Reward: 0.0\n",
      "Epoch 414/5000, [##----------------------------] Loss: 1.2838, Epsilon: 0.4366, Reward: 0.0\n",
      "Epoch 415/5000, [##----------------------------] Loss: 1.8517, Epsilon: 0.4357, Reward: 0.0\n",
      "Epoch 416/5000, [##----------------------------] Loss: 1.4260, Epsilon: 0.4348, Reward: 1600.0\n",
      "Epoch 417/5000, [##----------------------------] Loss: 1.2056, Epsilon: 0.4339, Reward: 0.0\n",
      "Epoch 418/5000, [##----------------------------] Loss: 1.5761, Epsilon: 0.4331, Reward: 0.0\n",
      "Epoch 419/5000, [##----------------------------] Loss: 0.9685, Epsilon: 0.4322, Reward: 0.0\n",
      "Epoch 420/5000, [##----------------------------] Loss: 0.9563, Epsilon: 0.4313, Reward: 0.0\n",
      "Epoch 421/5000, [##----------------------------] Loss: 1.3719, Epsilon: 0.4305, Reward: 0.0\n",
      "Epoch 422/5000, [##----------------------------] Loss: 1.0570, Epsilon: 0.4296, Reward: 0.0\n",
      "Epoch 423/5000, [##----------------------------] Loss: 0.7010, Epsilon: 0.4288, Reward: 0.0\n",
      "Epoch 424/5000, [##----------------------------] Loss: 0.9361, Epsilon: 0.4279, Reward: 0.0\n",
      "Epoch 425/5000, [##----------------------------] Loss: 0.7354, Epsilon: 0.4271, Reward: 0.0\n",
      "Epoch 426/5000, [##----------------------------] Loss: 0.4001, Epsilon: 0.4262, Reward: 0.0\n",
      "Epoch 427/5000, [##----------------------------] Loss: 0.1878, Epsilon: 0.4253, Reward: 0.0\n",
      "Epoch 428/5000, [##----------------------------] Loss: 0.0815, Epsilon: 0.4245, Reward: 0.0\n",
      "Epoch 429/5000, [##----------------------------] Loss: 0.0188, Epsilon: 0.4236, Reward: 1600.0\n",
      "Epoch 430/5000, [##----------------------------] Loss: 1.0202, Epsilon: 0.4228, Reward: 0.0\n",
      "Epoch 431/5000, [##----------------------------] Loss: 0.0440, Epsilon: 0.4220, Reward: 0.0\n",
      "Epoch 432/5000, [##----------------------------] Loss: 0.0620, Epsilon: 0.4211, Reward: 0.0\n",
      "Epoch 433/5000, [##----------------------------] Loss: 0.0811, Epsilon: 0.4203, Reward: 0.0\n",
      "Epoch 434/5000, [##----------------------------] Loss: 0.1563, Epsilon: 0.4194, Reward: 0.0\n",
      "Epoch 435/5000, [##----------------------------] Loss: 0.1603, Epsilon: 0.4186, Reward: 0.0\n",
      "Epoch 436/5000, [##----------------------------] Loss: 0.0198, Epsilon: 0.4177, Reward: 0.0\n",
      "Epoch 437/5000, [##----------------------------] Loss: 0.0224, Epsilon: 0.4169, Reward: 800.0\n",
      "Epoch 438/5000, [##----------------------------] Loss: 0.0698, Epsilon: 0.4161, Reward: 0.0\n",
      "Epoch 439/5000, [##----------------------------] Loss: 0.0251, Epsilon: 0.4152, Reward: 0.0\n",
      "Epoch 440/5000, [##----------------------------] Loss: 0.0927, Epsilon: 0.4144, Reward: 0.0\n",
      "Epoch 441/5000, [##----------------------------] Loss: 0.1532, Epsilon: 0.4136, Reward: 0.0\n",
      "Epoch 442/5000, [##----------------------------] Loss: 0.0923, Epsilon: 0.4128, Reward: 0.0\n",
      "Epoch 443/5000, [##----------------------------] Loss: 0.0412, Epsilon: 0.4119, Reward: 0.0\n",
      "Epoch 444/5000, [##----------------------------] Loss: 1.9429, Epsilon: 0.4111, Reward: 0.0\n",
      "Epoch 445/5000, [##----------------------------] Loss: 0.0357, Epsilon: 0.4103, Reward: 0.0\n",
      "Epoch 446/5000, [##----------------------------] Loss: 0.0018, Epsilon: 0.4095, Reward: 0.0\n",
      "Epoch 447/5000, [##----------------------------] Loss: 0.0005, Epsilon: 0.4087, Reward: 0.0\n",
      "Epoch 448/5000, [##----------------------------] Loss: 0.0806, Epsilon: 0.4078, Reward: 3200.0\n",
      "Epoch 449/5000, [##----------------------------] Loss: 0.4925, Epsilon: 0.4070, Reward: 800.0\n",
      "Epoch 450/5000, [##----------------------------] Loss: 1.0678, Epsilon: 0.4062, Reward: 2400.0\n",
      "Epoch 451/5000, [##----------------------------] Loss: 2.3239, Epsilon: 0.4054, Reward: 0.0\n",
      "Epoch 452/5000, [##----------------------------] Loss: 1.5245, Epsilon: 0.4046, Reward: 0.0\n",
      "Epoch 453/5000, [##----------------------------] Loss: 1.0156, Epsilon: 0.4038, Reward: 0.0\n",
      "Epoch 454/5000, [##----------------------------] Loss: 0.5063, Epsilon: 0.4030, Reward: 0.0\n",
      "Epoch 455/5000, [##----------------------------] Loss: 0.6655, Epsilon: 0.4022, Reward: 0.0\n",
      "Epoch 456/5000, [##----------------------------] Loss: 0.4884, Epsilon: 0.4014, Reward: 0.0\n",
      "Epoch 457/5000, [##----------------------------] Loss: 0.4308, Epsilon: 0.4006, Reward: 1600.0\n",
      "Epoch 458/5000, [##----------------------------] Loss: 0.3712, Epsilon: 0.3997, Reward: 0.0\n",
      "Epoch 459/5000, [##----------------------------] Loss: 0.3382, Epsilon: 0.3989, Reward: 800.0\n",
      "Epoch 460/5000, [##----------------------------] Loss: 0.3394, Epsilon: 0.3982, Reward: 0.0\n",
      "Epoch 461/5000, [##----------------------------] Loss: 0.5090, Epsilon: 0.3974, Reward: 0.0\n",
      "Epoch 462/5000, [##----------------------------] Loss: 1.0524, Epsilon: 0.3966, Reward: 2400.0\n",
      "Epoch 463/5000, [##----------------------------] Loss: 1.0433, Epsilon: 0.3958, Reward: 0.0\n",
      "Epoch 464/5000, [##----------------------------] Loss: 1.6264, Epsilon: 0.3950, Reward: 1600.0\n",
      "Epoch 465/5000, [##----------------------------] Loss: 0.8178, Epsilon: 0.3942, Reward: 0.0\n",
      "Epoch 466/5000, [##----------------------------] Loss: 1.8037, Epsilon: 0.3934, Reward: 0.0\n",
      "Epoch 467/5000, [##----------------------------] Loss: 19937.5723, Epsilon: 0.3926, Reward: 1600.0\n",
      "Epoch 468/5000, [##----------------------------] Loss: 0.6525, Epsilon: 0.3918, Reward: 0.0\n",
      "Epoch 469/5000, [##----------------------------] Loss: 0.6771, Epsilon: 0.3910, Reward: 1600.0\n",
      "Epoch 470/5000, [##----------------------------] Loss: 1.1372, Epsilon: 0.3903, Reward: 0.0\n",
      "Epoch 471/5000, [##----------------------------] Loss: 1.6538, Epsilon: 0.3895, Reward: 0.0\n",
      "Epoch 472/5000, [##----------------------------] Loss: 1.2587, Epsilon: 0.3887, Reward: 0.0\n",
      "Epoch 473/5000, [##----------------------------] Loss: 1.4315, Epsilon: 0.3879, Reward: 0.0\n",
      "Epoch 474/5000, [##----------------------------] Loss: 1.2274, Epsilon: 0.3871, Reward: 1600.0\n",
      "Epoch 475/5000, [##----------------------------] Loss: 2.1202, Epsilon: 0.3864, Reward: 0.0\n",
      "Epoch 476/5000, [##----------------------------] Loss: 1.3993, Epsilon: 0.3856, Reward: 0.0\n",
      "Epoch 477/5000, [##----------------------------] Loss: 1.4275, Epsilon: 0.3848, Reward: 0.0\n",
      "Epoch 478/5000, [##----------------------------] Loss: 1.5336, Epsilon: 0.3841, Reward: 0.0\n",
      "Epoch 479/5000, [##----------------------------] Loss: 0.9816, Epsilon: 0.3833, Reward: 0.0\n",
      "Epoch 480/5000, [##----------------------------] Loss: 0.6834, Epsilon: 0.3825, Reward: 0.0\n",
      "Epoch 481/5000, [##----------------------------] Loss: 0.2356, Epsilon: 0.3818, Reward: 0.0\n",
      "Epoch 482/5000, [##----------------------------] Loss: 0.2925, Epsilon: 0.3810, Reward: 0.0\n",
      "Epoch 483/5000, [##----------------------------] Loss: 0.2568, Epsilon: 0.3802, Reward: 0.0\n",
      "Epoch 484/5000, [##----------------------------] Loss: 0.0583, Epsilon: 0.3795, Reward: 0.0\n",
      "Epoch 485/5000, [##----------------------------] Loss: 0.0511, Epsilon: 0.3787, Reward: 0.0\n",
      "Epoch 486/5000, [##----------------------------] Loss: 0.0320, Epsilon: 0.3780, Reward: 0.0\n",
      "Epoch 487/5000, [##----------------------------] Loss: 0.0103, Epsilon: 0.3772, Reward: 0.0\n",
      "Epoch 488/5000, [##----------------------------] Loss: 0.0034, Epsilon: 0.3764, Reward: 0.0\n",
      "Epoch 489/5000, [##----------------------------] Loss: 0.0000, Epsilon: 0.3757, Reward: 0.0\n",
      "Epoch 490/5000, [##----------------------------] Loss: 0.0001, Epsilon: 0.3749, Reward: 0.0\n",
      "Epoch 491/5000, [##----------------------------] Loss: 0.0007, Epsilon: 0.3742, Reward: 0.0\n",
      "Epoch 492/5000, [##----------------------------] Loss: 0.0001, Epsilon: 0.3734, Reward: 0.0\n",
      "Epoch 493/5000, [##----------------------------] Loss: 0.0009, Epsilon: 0.3727, Reward: 0.0\n",
      "Epoch 494/5000, [##----------------------------] Loss: 0.0029, Epsilon: 0.3720, Reward: 0.0\n",
      "Epoch 495/5000, [##----------------------------] Loss: 0.0170, Epsilon: 0.3712, Reward: 1600.0\n",
      "Epoch 496/5000, [##----------------------------] Loss: 0.0389, Epsilon: 0.3705, Reward: 0.0\n",
      "Epoch 497/5000, [##----------------------------] Loss: 0.0264, Epsilon: 0.3697, Reward: 1600.0\n",
      "Epoch 498/5000, [##----------------------------] Loss: 0.2061, Epsilon: 0.3690, Reward: 0.0\n",
      "Epoch 499/5000, [##----------------------------] Loss: 0.3528, Epsilon: 0.3682, Reward: 0.0\n",
      "Epoch 500/5000, [###---------------------------] Loss: 0.2777, Epsilon: 0.3675, Reward: 0.0\n",
      "Epoch 501/5000, [###---------------------------] Loss: 0.0040, Epsilon: 0.3668, Reward: 0.0\n",
      "Epoch 502/5000, [###---------------------------] Loss: 0.0064, Epsilon: 0.3660, Reward: 0.0\n",
      "Epoch 503/5000, [###---------------------------] Loss: 0.0184, Epsilon: 0.3653, Reward: 0.0\n",
      "Epoch 504/5000, [###---------------------------] Loss: 0.0783, Epsilon: 0.3646, Reward: 0.0\n",
      "Epoch 505/5000, [###---------------------------] Loss: 0.0839, Epsilon: 0.3639, Reward: 1600.0\n",
      "Epoch 506/5000, [###---------------------------] Loss: 0.1184, Epsilon: 0.3631, Reward: 0.0\n",
      "Epoch 507/5000, [###---------------------------] Loss: 0.1304, Epsilon: 0.3624, Reward: 0.0\n",
      "Epoch 508/5000, [###---------------------------] Loss: 0.1333, Epsilon: 0.3617, Reward: 0.0\n",
      "Epoch 509/5000, [###---------------------------] Loss: 0.3091, Epsilon: 0.3609, Reward: 1600.0\n",
      "Epoch 510/5000, [###---------------------------] Loss: 0.4466, Epsilon: 0.3602, Reward: 1600.0\n",
      "Epoch 511/5000, [###---------------------------] Loss: 19916.2656, Epsilon: 0.3595, Reward: 1600.0\n",
      "Epoch 512/5000, [###---------------------------] Loss: 0.9987, Epsilon: 0.3588, Reward: 0.0\n",
      "Epoch 513/5000, [###---------------------------] Loss: 0.4955, Epsilon: 0.3581, Reward: 0.0\n",
      "Epoch 514/5000, [###---------------------------] Loss: 0.4550, Epsilon: 0.3574, Reward: 0.0\n",
      "Epoch 515/5000, [###---------------------------] Loss: 2.5773, Epsilon: 0.3566, Reward: 800.0\n",
      "Epoch 516/5000, [###---------------------------] Loss: 1.1223, Epsilon: 0.3559, Reward: 1600.0\n",
      "Epoch 517/5000, [###---------------------------] Loss: 0.7277, Epsilon: 0.3552, Reward: 0.0\n",
      "Epoch 518/5000, [###---------------------------] Loss: 1.9448, Epsilon: 0.3545, Reward: 1600.0\n",
      "Epoch 519/5000, [###---------------------------] Loss: 0.8140, Epsilon: 0.3538, Reward: 0.0\n",
      "Epoch 520/5000, [###---------------------------] Loss: 1.1115, Epsilon: 0.3531, Reward: 800.0\n",
      "Epoch 521/5000, [###---------------------------] Loss: 1.5894, Epsilon: 0.3524, Reward: 0.0\n",
      "Epoch 522/5000, [###---------------------------] Loss: 1.1415, Epsilon: 0.3517, Reward: 0.0\n",
      "Epoch 523/5000, [###---------------------------] Loss: 0.8518, Epsilon: 0.3510, Reward: 0.0\n",
      "Epoch 524/5000, [###---------------------------] Loss: 0.4903, Epsilon: 0.3503, Reward: 0.0\n",
      "Epoch 525/5000, [###---------------------------] Loss: 0.6174, Epsilon: 0.3496, Reward: 0.0\n",
      "Epoch 526/5000, [###---------------------------] Loss: 0.5932, Epsilon: 0.3489, Reward: 0.0\n",
      "Epoch 527/5000, [###---------------------------] Loss: 0.2371, Epsilon: 0.3482, Reward: 800.0\n",
      "Epoch 528/5000, [###---------------------------] Loss: 0.5594, Epsilon: 0.3475, Reward: 0.0\n",
      "Epoch 529/5000, [###---------------------------] Loss: 0.4504, Epsilon: 0.3468, Reward: 0.0\n",
      "Epoch 530/5000, [###---------------------------] Loss: 0.4040, Epsilon: 0.3461, Reward: 0.0\n",
      "Epoch 531/5000, [###---------------------------] Loss: 0.4385, Epsilon: 0.3454, Reward: 0.0\n",
      "Epoch 532/5000, [###---------------------------] Loss: 0.2821, Epsilon: 0.3447, Reward: 1600.0\n",
      "Epoch 533/5000, [###---------------------------] Loss: 0.2990, Epsilon: 0.3440, Reward: 800.0\n",
      "Epoch 534/5000, [###---------------------------] Loss: 0.1099, Epsilon: 0.3433, Reward: 0.0\n",
      "Epoch 535/5000, [###---------------------------] Loss: 0.0559, Epsilon: 0.3426, Reward: 0.0\n",
      "Epoch 536/5000, [###---------------------------] Loss: 0.1625, Epsilon: 0.3420, Reward: 0.0\n",
      "Epoch 537/5000, [###---------------------------] Loss: 0.1277, Epsilon: 0.3413, Reward: 0.0\n",
      "Epoch 538/5000, [###---------------------------] Loss: 0.3881, Epsilon: 0.3406, Reward: 1600.0\n",
      "Epoch 539/5000, [###---------------------------] Loss: 0.2715, Epsilon: 0.3399, Reward: 0.0\n",
      "Epoch 540/5000, [###---------------------------] Loss: 0.5128, Epsilon: 0.3392, Reward: 0.0\n",
      "Epoch 541/5000, [###---------------------------] Loss: 0.9575, Epsilon: 0.3386, Reward: 0.0\n",
      "Epoch 542/5000, [###---------------------------] Loss: 0.4054, Epsilon: 0.3379, Reward: 0.0\n",
      "Epoch 543/5000, [###---------------------------] Loss: 0.3049, Epsilon: 0.3372, Reward: 0.0\n",
      "Epoch 544/5000, [###---------------------------] Loss: 0.4248, Epsilon: 0.3365, Reward: 0.0\n",
      "Epoch 545/5000, [###---------------------------] Loss: 0.6086, Epsilon: 0.3358, Reward: 0.0\n",
      "Epoch 546/5000, [###---------------------------] Loss: 0.7286, Epsilon: 0.3352, Reward: 0.0\n",
      "Epoch 547/5000, [###---------------------------] Loss: 0.3037, Epsilon: 0.3345, Reward: 0.0\n",
      "Epoch 548/5000, [###---------------------------] Loss: 0.1186, Epsilon: 0.3338, Reward: 800.0\n",
      "Epoch 549/5000, [###---------------------------] Loss: 0.0476, Epsilon: 0.3332, Reward: 0.0\n",
      "Epoch 550/5000, [###---------------------------] Loss: 1.3382, Epsilon: 0.3325, Reward: 1600.0\n",
      "Epoch 551/5000, [###---------------------------] Loss: 0.0976, Epsilon: 0.3318, Reward: 0.0\n",
      "Epoch 552/5000, [###---------------------------] Loss: 0.1019, Epsilon: 0.3312, Reward: 0.0\n",
      "Epoch 553/5000, [###---------------------------] Loss: 0.1451, Epsilon: 0.3305, Reward: 0.0\n",
      "Epoch 554/5000, [###---------------------------] Loss: 0.0962, Epsilon: 0.3299, Reward: 1600.0\n",
      "Epoch 555/5000, [###---------------------------] Loss: 0.1836, Epsilon: 0.3292, Reward: 0.0\n",
      "Epoch 556/5000, [###---------------------------] Loss: 0.1392, Epsilon: 0.3285, Reward: 0.0\n",
      "Epoch 557/5000, [###---------------------------] Loss: 0.1654, Epsilon: 0.3279, Reward: 1600.0\n",
      "Epoch 558/5000, [###---------------------------] Loss: 0.4316, Epsilon: 0.3272, Reward: 0.0\n",
      "Epoch 559/5000, [###---------------------------] Loss: 0.9248, Epsilon: 0.3266, Reward: 0.0\n",
      "Epoch 560/5000, [###---------------------------] Loss: 0.5382, Epsilon: 0.3259, Reward: 800.0\n",
      "Epoch 561/5000, [###---------------------------] Loss: 0.3421, Epsilon: 0.3253, Reward: 0.0\n",
      "Epoch 562/5000, [###---------------------------] Loss: 0.8577, Epsilon: 0.3246, Reward: 0.0\n",
      "Epoch 563/5000, [###---------------------------] Loss: 0.6012, Epsilon: 0.3240, Reward: 2400.0\n",
      "Epoch 564/5000, [###---------------------------] Loss: 0.8385, Epsilon: 0.3233, Reward: 0.0\n",
      "Epoch 565/5000, [###---------------------------] Loss: 1.0464, Epsilon: 0.3227, Reward: 0.0\n",
      "Epoch 566/5000, [###---------------------------] Loss: 0.6819, Epsilon: 0.3220, Reward: 0.0\n",
      "Epoch 567/5000, [###---------------------------] Loss: 0.5785, Epsilon: 0.3214, Reward: 800.0\n",
      "Epoch 568/5000, [###---------------------------] Loss: 0.4087, Epsilon: 0.3207, Reward: 0.0\n",
      "Epoch 569/5000, [###---------------------------] Loss: 0.9870, Epsilon: 0.3201, Reward: 800.0\n",
      "Epoch 570/5000, [###---------------------------] Loss: 1.0251, Epsilon: 0.3195, Reward: 1600.0\n",
      "Epoch 571/5000, [###---------------------------] Loss: 1.0068, Epsilon: 0.3188, Reward: 0.0\n",
      "Epoch 572/5000, [###---------------------------] Loss: 1.1258, Epsilon: 0.3182, Reward: 0.0\n",
      "Epoch 573/5000, [###---------------------------] Loss: 1.4974, Epsilon: 0.3175, Reward: 0.0\n",
      "Epoch 574/5000, [###---------------------------] Loss: 0.8422, Epsilon: 0.3169, Reward: 0.0\n",
      "Epoch 575/5000, [###---------------------------] Loss: 0.5380, Epsilon: 0.3163, Reward: 0.0\n",
      "Epoch 576/5000, [###---------------------------] Loss: 0.5240, Epsilon: 0.3156, Reward: 800.0\n",
      "Epoch 577/5000, [###---------------------------] Loss: 0.2456, Epsilon: 0.3150, Reward: 0.0\n",
      "Epoch 578/5000, [###---------------------------] Loss: 0.2951, Epsilon: 0.3144, Reward: 0.0\n",
      "Epoch 579/5000, [###---------------------------] Loss: 0.1991, Epsilon: 0.3137, Reward: 0.0\n",
      "Epoch 580/5000, [###---------------------------] Loss: 0.1666, Epsilon: 0.3131, Reward: 0.0\n",
      "Epoch 581/5000, [###---------------------------] Loss: 0.0817, Epsilon: 0.3125, Reward: 0.0\n",
      "Epoch 582/5000, [###---------------------------] Loss: 0.0656, Epsilon: 0.3119, Reward: 0.0\n",
      "Epoch 583/5000, [###---------------------------] Loss: 0.2883, Epsilon: 0.3112, Reward: 1600.0\n",
      "Epoch 584/5000, [###---------------------------] Loss: 0.2469, Epsilon: 0.3106, Reward: 0.0\n",
      "Epoch 585/5000, [###---------------------------] Loss: 0.3324, Epsilon: 0.3100, Reward: 1600.0\n",
      "Epoch 586/5000, [###---------------------------] Loss: 0.3363, Epsilon: 0.3094, Reward: 1600.0\n",
      "Epoch 587/5000, [###---------------------------] Loss: 0.3810, Epsilon: 0.3088, Reward: 0.0\n",
      "Epoch 588/5000, [###---------------------------] Loss: 0.3540, Epsilon: 0.3081, Reward: 0.0\n",
      "Epoch 589/5000, [###---------------------------] Loss: 0.2275, Epsilon: 0.3075, Reward: 0.0\n",
      "Epoch 590/5000, [###---------------------------] Loss: 0.6350, Epsilon: 0.3069, Reward: 0.0\n",
      "Epoch 591/5000, [###---------------------------] Loss: 0.6375, Epsilon: 0.3063, Reward: 0.0\n",
      "Epoch 592/5000, [###---------------------------] Loss: 0.6658, Epsilon: 0.3057, Reward: 0.0\n",
      "Epoch 593/5000, [###---------------------------] Loss: 0.3885, Epsilon: 0.3051, Reward: 0.0\n",
      "Epoch 594/5000, [###---------------------------] Loss: 0.3136, Epsilon: 0.3045, Reward: 0.0\n",
      "Epoch 595/5000, [###---------------------------] Loss: 0.3578, Epsilon: 0.3039, Reward: 1600.0\n",
      "Epoch 596/5000, [###---------------------------] Loss: 1.1617, Epsilon: 0.3033, Reward: 0.0\n",
      "Epoch 597/5000, [###---------------------------] Loss: 0.9877, Epsilon: 0.3026, Reward: 0.0\n",
      "Epoch 598/5000, [###---------------------------] Loss: 0.4304, Epsilon: 0.3020, Reward: 800.0\n",
      "Epoch 599/5000, [###---------------------------] Loss: 1.0149, Epsilon: 0.3014, Reward: 1600.0\n",
      "Epoch 600/5000, [###---------------------------] Loss: 0.4525, Epsilon: 0.3008, Reward: 0.0\n",
      "Epoch 601/5000, [###---------------------------] Loss: 1.9388, Epsilon: 0.3002, Reward: 0.0\n",
      "Epoch 602/5000, [###---------------------------] Loss: 0.1982, Epsilon: 0.2996, Reward: 0.0\n",
      "Epoch 603/5000, [###---------------------------] Loss: 0.3079, Epsilon: 0.2990, Reward: 1600.0\n",
      "Epoch 604/5000, [###---------------------------] Loss: 0.2785, Epsilon: 0.2984, Reward: 0.0\n",
      "Epoch 605/5000, [###---------------------------] Loss: 0.5069, Epsilon: 0.2978, Reward: 0.0\n",
      "Epoch 606/5000, [###---------------------------] Loss: 0.2793, Epsilon: 0.2972, Reward: 0.0\n",
      "Epoch 607/5000, [###---------------------------] Loss: 0.1544, Epsilon: 0.2966, Reward: 1600.0\n",
      "Epoch 608/5000, [###---------------------------] Loss: 0.2658, Epsilon: 0.2961, Reward: 1600.0\n",
      "Epoch 609/5000, [###---------------------------] Loss: 0.4482, Epsilon: 0.2955, Reward: 1600.0\n",
      "Epoch 610/5000, [###---------------------------] Loss: 0.7331, Epsilon: 0.2949, Reward: 0.0\n",
      "Epoch 611/5000, [###---------------------------] Loss: 0.6923, Epsilon: 0.2943, Reward: 0.0\n",
      "Epoch 612/5000, [###---------------------------] Loss: 0.6141, Epsilon: 0.2937, Reward: 0.0\n",
      "Epoch 613/5000, [###---------------------------] Loss: 0.5941, Epsilon: 0.2931, Reward: 0.0\n",
      "Epoch 614/5000, [###---------------------------] Loss: 0.6273, Epsilon: 0.2925, Reward: 0.0\n",
      "Epoch 615/5000, [###---------------------------] Loss: 0.8629, Epsilon: 0.2919, Reward: 0.0\n",
      "Epoch 616/5000, [###---------------------------] Loss: 2.5271, Epsilon: 0.2913, Reward: 0.0\n",
      "Epoch 617/5000, [###---------------------------] Loss: 0.6093, Epsilon: 0.2908, Reward: 0.0\n",
      "Epoch 618/5000, [###---------------------------] Loss: 0.3665, Epsilon: 0.2902, Reward: 0.0\n",
      "Epoch 619/5000, [###---------------------------] Loss: 0.3448, Epsilon: 0.2896, Reward: 0.0\n",
      "Epoch 620/5000, [###---------------------------] Loss: 0.6863, Epsilon: 0.2890, Reward: 2400.0\n",
      "Epoch 621/5000, [###---------------------------] Loss: 1.0743, Epsilon: 0.2884, Reward: 1600.0\n",
      "Epoch 622/5000, [###---------------------------] Loss: 1.0026, Epsilon: 0.2879, Reward: 1600.0\n",
      "Epoch 623/5000, [###---------------------------] Loss: 1.0850, Epsilon: 0.2873, Reward: 1600.0\n",
      "Epoch 624/5000, [###---------------------------] Loss: 1.4685, Epsilon: 0.2867, Reward: 800.0\n",
      "Epoch 625/5000, [###---------------------------] Loss: 1.0081, Epsilon: 0.2861, Reward: 0.0\n",
      "Epoch 626/5000, [###---------------------------] Loss: 0.8952, Epsilon: 0.2856, Reward: 0.0\n",
      "Epoch 627/5000, [###---------------------------] Loss: 0.6363, Epsilon: 0.2850, Reward: 0.0\n",
      "Epoch 628/5000, [###---------------------------] Loss: 2.8858, Epsilon: 0.2844, Reward: 0.0\n",
      "Epoch 629/5000, [###---------------------------] Loss: 1.9228, Epsilon: 0.2839, Reward: 800.0\n",
      "Epoch 630/5000, [###---------------------------] Loss: 1.0678, Epsilon: 0.2833, Reward: 0.0\n",
      "Epoch 631/5000, [###---------------------------] Loss: 0.8959, Epsilon: 0.2827, Reward: 0.0\n",
      "Epoch 632/5000, [###---------------------------] Loss: 0.3481, Epsilon: 0.2822, Reward: 1600.0\n",
      "Epoch 633/5000, [###---------------------------] Loss: 0.7438, Epsilon: 0.2816, Reward: 0.0\n",
      "Epoch 634/5000, [###---------------------------] Loss: 0.6688, Epsilon: 0.2810, Reward: 800.0\n",
      "Epoch 635/5000, [###---------------------------] Loss: 0.6824, Epsilon: 0.2805, Reward: 0.0\n",
      "Epoch 636/5000, [###---------------------------] Loss: 0.2495, Epsilon: 0.2799, Reward: 0.0\n",
      "Epoch 637/5000, [###---------------------------] Loss: 0.2311, Epsilon: 0.2794, Reward: 0.0\n",
      "Epoch 638/5000, [###---------------------------] Loss: 0.2508, Epsilon: 0.2788, Reward: 0.0\n",
      "Epoch 639/5000, [###---------------------------] Loss: 0.3703, Epsilon: 0.2782, Reward: 1600.0\n",
      "Epoch 640/5000, [###---------------------------] Loss: 0.3227, Epsilon: 0.2777, Reward: 0.0\n",
      "Epoch 641/5000, [###---------------------------] Loss: 0.8342, Epsilon: 0.2771, Reward: 0.0\n",
      "Epoch 642/5000, [###---------------------------] Loss: 0.7317, Epsilon: 0.2766, Reward: 0.0\n",
      "Epoch 643/5000, [###---------------------------] Loss: 0.8405, Epsilon: 0.2760, Reward: 1600.0\n",
      "Epoch 644/5000, [###---------------------------] Loss: 0.9135, Epsilon: 0.2755, Reward: 0.0\n",
      "Epoch 645/5000, [###---------------------------] Loss: 0.7250, Epsilon: 0.2749, Reward: 0.0\n",
      "Epoch 646/5000, [###---------------------------] Loss: 0.5375, Epsilon: 0.2744, Reward: 0.0\n",
      "Epoch 647/5000, [###---------------------------] Loss: 0.2405, Epsilon: 0.2738, Reward: 0.0\n",
      "Epoch 648/5000, [###---------------------------] Loss: 0.4879, Epsilon: 0.2733, Reward: 0.0\n",
      "Epoch 649/5000, [###---------------------------] Loss: 0.3996, Epsilon: 0.2727, Reward: 800.0\n",
      "Epoch 650/5000, [###---------------------------] Loss: 0.0751, Epsilon: 0.2722, Reward: 0.0\n",
      "Epoch 651/5000, [###---------------------------] Loss: 0.2917, Epsilon: 0.2716, Reward: 0.0\n",
      "Epoch 652/5000, [###---------------------------] Loss: 0.2814, Epsilon: 0.2711, Reward: 0.0\n",
      "Epoch 653/5000, [###---------------------------] Loss: 0.1340, Epsilon: 0.2705, Reward: 1600.0\n",
      "Epoch 654/5000, [###---------------------------] Loss: 0.0754, Epsilon: 0.2700, Reward: 0.0\n",
      "Epoch 655/5000, [###---------------------------] Loss: 0.4969, Epsilon: 0.2695, Reward: 1600.0\n",
      "Epoch 656/5000, [###---------------------------] Loss: 0.2280, Epsilon: 0.2689, Reward: 0.0\n",
      "Epoch 657/5000, [###---------------------------] Loss: 0.2816, Epsilon: 0.2684, Reward: 1600.0\n",
      "Epoch 658/5000, [###---------------------------] Loss: 19967.2559, Epsilon: 0.2679, Reward: 800.0\n",
      "Epoch 659/5000, [###---------------------------] Loss: 0.3855, Epsilon: 0.2673, Reward: 0.0\n",
      "Epoch 660/5000, [###---------------------------] Loss: 0.3646, Epsilon: 0.2668, Reward: 0.0\n",
      "Epoch 661/5000, [###---------------------------] Loss: 0.9900, Epsilon: 0.2662, Reward: 1600.0\n",
      "Epoch 662/5000, [###---------------------------] Loss: 19924.9004, Epsilon: 0.2657, Reward: 1600.0\n",
      "Epoch 663/5000, [###---------------------------] Loss: 1.2971, Epsilon: 0.2652, Reward: 0.0\n",
      "Epoch 664/5000, [###---------------------------] Loss: 0.9995, Epsilon: 0.2647, Reward: 0.0\n",
      "Epoch 665/5000, [###---------------------------] Loss: 1.4905, Epsilon: 0.2641, Reward: 0.0\n",
      "Epoch 666/5000, [###---------------------------] Loss: 0.9379, Epsilon: 0.2636, Reward: 0.0\n",
      "Epoch 667/5000, [####--------------------------] Loss: 0.6692, Epsilon: 0.2631, Reward: 1600.0\n",
      "Epoch 668/5000, [####--------------------------] Loss: 0.9140, Epsilon: 0.2625, Reward: 0.0\n",
      "Epoch 669/5000, [####--------------------------] Loss: 1.3602, Epsilon: 0.2620, Reward: 800.0\n",
      "Epoch 670/5000, [####--------------------------] Loss: 1.1071, Epsilon: 0.2615, Reward: 0.0\n",
      "Epoch 671/5000, [####--------------------------] Loss: 0.8791, Epsilon: 0.2610, Reward: 1600.0\n",
      "Epoch 672/5000, [####--------------------------] Loss: 0.6416, Epsilon: 0.2604, Reward: 1600.0\n",
      "Epoch 673/5000, [####--------------------------] Loss: 0.8580, Epsilon: 0.2599, Reward: 0.0\n",
      "Epoch 674/5000, [####--------------------------] Loss: 1.5053, Epsilon: 0.2594, Reward: 0.0\n",
      "Epoch 675/5000, [####--------------------------] Loss: 0.9355, Epsilon: 0.2589, Reward: 0.0\n",
      "Epoch 676/5000, [####--------------------------] Loss: 0.5366, Epsilon: 0.2584, Reward: 0.0\n",
      "Epoch 677/5000, [####--------------------------] Loss: 0.5087, Epsilon: 0.2579, Reward: 0.0\n",
      "Epoch 678/5000, [####--------------------------] Loss: 0.5244, Epsilon: 0.2573, Reward: 800.0\n",
      "Epoch 679/5000, [####--------------------------] Loss: 0.6911, Epsilon: 0.2568, Reward: 0.0\n",
      "Epoch 680/5000, [####--------------------------] Loss: 0.6720, Epsilon: 0.2563, Reward: 1600.0\n",
      "Epoch 681/5000, [####--------------------------] Loss: 0.8557, Epsilon: 0.2558, Reward: 0.0\n",
      "Epoch 682/5000, [####--------------------------] Loss: 1.0665, Epsilon: 0.2553, Reward: 0.0\n",
      "Epoch 683/5000, [####--------------------------] Loss: 0.8259, Epsilon: 0.2548, Reward: 1600.0\n",
      "Epoch 684/5000, [####--------------------------] Loss: 1.6689, Epsilon: 0.2543, Reward: 0.0\n",
      "Epoch 685/5000, [####--------------------------] Loss: 0.9619, Epsilon: 0.2538, Reward: 0.0\n",
      "Epoch 686/5000, [####--------------------------] Loss: 0.7597, Epsilon: 0.2533, Reward: 0.0\n",
      "Epoch 687/5000, [####--------------------------] Loss: 0.3869, Epsilon: 0.2527, Reward: 0.0\n",
      "Epoch 688/5000, [####--------------------------] Loss: 0.8353, Epsilon: 0.2522, Reward: 0.0\n",
      "Epoch 689/5000, [####--------------------------] Loss: 0.7310, Epsilon: 0.2517, Reward: 1600.0\n",
      "Epoch 690/5000, [####--------------------------] Loss: 0.9882, Epsilon: 0.2512, Reward: 1600.0\n",
      "Epoch 691/5000, [####--------------------------] Loss: 1.0358, Epsilon: 0.2507, Reward: 800.0\n",
      "Epoch 692/5000, [####--------------------------] Loss: 1.2350, Epsilon: 0.2502, Reward: 800.0\n",
      "Epoch 693/5000, [####--------------------------] Loss: 0.6450, Epsilon: 0.2497, Reward: 800.0\n",
      "Epoch 694/5000, [####--------------------------] Loss: 0.8616, Epsilon: 0.2492, Reward: 0.0\n",
      "Epoch 695/5000, [####--------------------------] Loss: 0.8950, Epsilon: 0.2487, Reward: 800.0\n",
      "Epoch 696/5000, [####--------------------------] Loss: 1.0694, Epsilon: 0.2482, Reward: 2400.0\n",
      "Epoch 697/5000, [####--------------------------] Loss: 1.1540, Epsilon: 0.2477, Reward: 800.0\n",
      "Epoch 698/5000, [####--------------------------] Loss: 1.2728, Epsilon: 0.2472, Reward: 0.0\n",
      "Epoch 699/5000, [####--------------------------] Loss: 1.6560, Epsilon: 0.2467, Reward: 0.0\n",
      "Epoch 700/5000, [####--------------------------] Loss: 1.9138, Epsilon: 0.2463, Reward: 0.0\n",
      "Epoch 701/5000, [####--------------------------] Loss: 0.3149, Epsilon: 0.2458, Reward: 0.0\n",
      "Epoch 702/5000, [####--------------------------] Loss: 1.1056, Epsilon: 0.2453, Reward: 0.0\n",
      "Epoch 703/5000, [####--------------------------] Loss: 1.1116, Epsilon: 0.2448, Reward: 0.0\n",
      "Epoch 704/5000, [####--------------------------] Loss: 19949.0820, Epsilon: 0.2443, Reward: 800.0\n",
      "Epoch 705/5000, [####--------------------------] Loss: 1.2681, Epsilon: 0.2438, Reward: 0.0\n",
      "Epoch 706/5000, [####--------------------------] Loss: 0.9387, Epsilon: 0.2433, Reward: 0.0\n",
      "Epoch 707/5000, [####--------------------------] Loss: 0.5131, Epsilon: 0.2428, Reward: 800.0\n",
      "Epoch 708/5000, [####--------------------------] Loss: 1.3171, Epsilon: 0.2423, Reward: 0.0\n",
      "Epoch 709/5000, [####--------------------------] Loss: 0.4477, Epsilon: 0.2419, Reward: 0.0\n",
      "Epoch 710/5000, [####--------------------------] Loss: 0.1473, Epsilon: 0.2414, Reward: 0.0\n",
      "Epoch 711/5000, [####--------------------------] Loss: 0.1291, Epsilon: 0.2409, Reward: 800.0\n",
      "Epoch 712/5000, [####--------------------------] Loss: 0.2576, Epsilon: 0.2404, Reward: 800.0\n",
      "Epoch 713/5000, [####--------------------------] Loss: 0.4359, Epsilon: 0.2399, Reward: 0.0\n",
      "Epoch 714/5000, [####--------------------------] Loss: 0.2699, Epsilon: 0.2394, Reward: 1600.0\n",
      "Epoch 715/5000, [####--------------------------] Loss: 0.3033, Epsilon: 0.2390, Reward: 0.0\n",
      "Epoch 716/5000, [####--------------------------] Loss: 0.4142, Epsilon: 0.2385, Reward: 0.0\n",
      "Epoch 717/5000, [####--------------------------] Loss: 0.5951, Epsilon: 0.2380, Reward: 0.0\n",
      "Epoch 718/5000, [####--------------------------] Loss: 0.6387, Epsilon: 0.2375, Reward: 0.0\n",
      "Epoch 719/5000, [####--------------------------] Loss: 0.3470, Epsilon: 0.2371, Reward: 800.0\n",
      "Epoch 720/5000, [####--------------------------] Loss: 0.3036, Epsilon: 0.2366, Reward: 0.0\n",
      "Epoch 721/5000, [####--------------------------] Loss: 0.2813, Epsilon: 0.2361, Reward: 0.0\n",
      "Epoch 722/5000, [####--------------------------] Loss: 0.3462, Epsilon: 0.2356, Reward: 1600.0\n",
      "Epoch 723/5000, [####--------------------------] Loss: 0.3488, Epsilon: 0.2352, Reward: 0.0\n",
      "Epoch 724/5000, [####--------------------------] Loss: 0.4111, Epsilon: 0.2347, Reward: 0.0\n",
      "Epoch 725/5000, [####--------------------------] Loss: 0.4119, Epsilon: 0.2342, Reward: 0.0\n",
      "Epoch 726/5000, [####--------------------------] Loss: 0.2891, Epsilon: 0.2338, Reward: 0.0\n",
      "Epoch 727/5000, [####--------------------------] Loss: 0.2627, Epsilon: 0.2333, Reward: 1600.0\n",
      "Epoch 728/5000, [####--------------------------] Loss: 0.3950, Epsilon: 0.2328, Reward: 0.0\n",
      "Epoch 729/5000, [####--------------------------] Loss: 3.4969, Epsilon: 0.2324, Reward: 0.0\n",
      "Epoch 730/5000, [####--------------------------] Loss: 0.2670, Epsilon: 0.2319, Reward: 0.0\n",
      "Epoch 731/5000, [####--------------------------] Loss: 0.2595, Epsilon: 0.2314, Reward: 0.0\n",
      "Epoch 732/5000, [####--------------------------] Loss: 0.3527, Epsilon: 0.2310, Reward: 800.0\n",
      "Epoch 733/5000, [####--------------------------] Loss: 0.4957, Epsilon: 0.2305, Reward: 0.0\n",
      "Epoch 734/5000, [####--------------------------] Loss: 0.3868, Epsilon: 0.2300, Reward: 0.0\n",
      "Epoch 735/5000, [####--------------------------] Loss: 0.2874, Epsilon: 0.2296, Reward: 0.0\n",
      "Epoch 736/5000, [####--------------------------] Loss: 0.3147, Epsilon: 0.2291, Reward: 0.0\n",
      "Epoch 737/5000, [####--------------------------] Loss: 0.2373, Epsilon: 0.2287, Reward: 0.0\n",
      "Epoch 738/5000, [####--------------------------] Loss: 0.1862, Epsilon: 0.2282, Reward: 0.0\n",
      "Epoch 739/5000, [####--------------------------] Loss: 2.9914, Epsilon: 0.2278, Reward: 0.0\n",
      "Epoch 740/5000, [####--------------------------] Loss: 19980.2754, Epsilon: 0.2273, Reward: 0.0\n",
      "Epoch 741/5000, [####--------------------------] Loss: 0.1955, Epsilon: 0.2268, Reward: 0.0\n",
      "Epoch 742/5000, [####--------------------------] Loss: 0.0471, Epsilon: 0.2264, Reward: 0.0\n",
      "Epoch 743/5000, [####--------------------------] Loss: 0.0387, Epsilon: 0.2259, Reward: 1600.0\n",
      "Epoch 744/5000, [####--------------------------] Loss: 0.0315, Epsilon: 0.2255, Reward: 0.0\n",
      "Epoch 745/5000, [####--------------------------] Loss: 0.0321, Epsilon: 0.2250, Reward: 1600.0\n",
      "Epoch 746/5000, [####--------------------------] Loss: 0.2424, Epsilon: 0.2246, Reward: 0.0\n",
      "Epoch 747/5000, [####--------------------------] Loss: 0.3130, Epsilon: 0.2241, Reward: 800.0\n",
      "Epoch 748/5000, [####--------------------------] Loss: 0.1967, Epsilon: 0.2237, Reward: 0.0\n",
      "Epoch 749/5000, [####--------------------------] Loss: 0.4004, Epsilon: 0.2232, Reward: 800.0\n",
      "Epoch 750/5000, [####--------------------------] Loss: 0.3131, Epsilon: 0.2228, Reward: 0.0\n",
      "Epoch 751/5000, [####--------------------------] Loss: 1.0733, Epsilon: 0.2223, Reward: 0.0\n",
      "Epoch 752/5000, [####--------------------------] Loss: 0.5140, Epsilon: 0.2219, Reward: 0.0\n",
      "Epoch 753/5000, [####--------------------------] Loss: 0.4944, Epsilon: 0.2215, Reward: 0.0\n",
      "Epoch 754/5000, [####--------------------------] Loss: 0.3262, Epsilon: 0.2210, Reward: 0.0\n",
      "Epoch 755/5000, [####--------------------------] Loss: 0.2120, Epsilon: 0.2206, Reward: 2400.0\n",
      "Epoch 756/5000, [####--------------------------] Loss: 19956.2109, Epsilon: 0.2201, Reward: 0.0\n",
      "Epoch 757/5000, [####--------------------------] Loss: 0.5616, Epsilon: 0.2197, Reward: 0.0\n",
      "Epoch 758/5000, [####--------------------------] Loss: 0.4499, Epsilon: 0.2193, Reward: 800.0\n",
      "Epoch 759/5000, [####--------------------------] Loss: 0.4155, Epsilon: 0.2188, Reward: 0.0\n",
      "Epoch 760/5000, [####--------------------------] Loss: 0.6363, Epsilon: 0.2184, Reward: 1600.0\n",
      "Epoch 761/5000, [####--------------------------] Loss: 0.1611, Epsilon: 0.2179, Reward: 0.0\n",
      "Epoch 762/5000, [####--------------------------] Loss: 0.2916, Epsilon: 0.2175, Reward: 800.0\n",
      "Epoch 763/5000, [####--------------------------] Loss: 0.5907, Epsilon: 0.2171, Reward: 0.0\n",
      "Epoch 764/5000, [####--------------------------] Loss: 0.5935, Epsilon: 0.2166, Reward: 0.0\n",
      "Epoch 765/5000, [####--------------------------] Loss: 0.9272, Epsilon: 0.2162, Reward: 1600.0\n",
      "Epoch 766/5000, [####--------------------------] Loss: 0.4872, Epsilon: 0.2158, Reward: 0.0\n",
      "Epoch 767/5000, [####--------------------------] Loss: 0.5039, Epsilon: 0.2153, Reward: 0.0\n",
      "Epoch 768/5000, [####--------------------------] Loss: 0.3572, Epsilon: 0.2149, Reward: 0.0\n",
      "Epoch 769/5000, [####--------------------------] Loss: 0.5091, Epsilon: 0.2145, Reward: 0.0\n",
      "Epoch 770/5000, [####--------------------------] Loss: 0.3993, Epsilon: 0.2141, Reward: 0.0\n",
      "Epoch 771/5000, [####--------------------------] Loss: 0.2016, Epsilon: 0.2136, Reward: 1600.0\n",
      "Epoch 772/5000, [####--------------------------] Loss: 0.6086, Epsilon: 0.2132, Reward: 0.0\n",
      "Epoch 773/5000, [####--------------------------] Loss: 0.7761, Epsilon: 0.2128, Reward: 0.0\n",
      "Epoch 774/5000, [####--------------------------] Loss: 0.2604, Epsilon: 0.2123, Reward: 0.0\n",
      "Epoch 775/5000, [####--------------------------] Loss: 3.3472, Epsilon: 0.2119, Reward: 0.0\n",
      "Epoch 776/5000, [####--------------------------] Loss: 0.2256, Epsilon: 0.2115, Reward: 1600.0\n",
      "Epoch 777/5000, [####--------------------------] Loss: 0.3807, Epsilon: 0.2111, Reward: 0.0\n",
      "Epoch 778/5000, [####--------------------------] Loss: 0.3360, Epsilon: 0.2106, Reward: 0.0\n",
      "Epoch 779/5000, [####--------------------------] Loss: 0.2772, Epsilon: 0.2102, Reward: 800.0\n",
      "Epoch 780/5000, [####--------------------------] Loss: 0.4394, Epsilon: 0.2098, Reward: 0.0\n",
      "Epoch 781/5000, [####--------------------------] Loss: 0.4169, Epsilon: 0.2094, Reward: 1600.0\n",
      "Epoch 782/5000, [####--------------------------] Loss: 0.7116, Epsilon: 0.2090, Reward: 0.0\n",
      "Epoch 783/5000, [####--------------------------] Loss: 0.6203, Epsilon: 0.2086, Reward: 0.0\n",
      "Epoch 784/5000, [####--------------------------] Loss: 1.2624, Epsilon: 0.2081, Reward: 1600.0\n",
      "Epoch 785/5000, [####--------------------------] Loss: 0.9928, Epsilon: 0.2077, Reward: 0.0\n",
      "Epoch 786/5000, [####--------------------------] Loss: 0.7452, Epsilon: 0.2073, Reward: 0.0\n",
      "Epoch 787/5000, [####--------------------------] Loss: 0.6561, Epsilon: 0.2069, Reward: 1600.0\n",
      "Epoch 788/5000, [####--------------------------] Loss: 0.8713, Epsilon: 0.2065, Reward: 0.0\n",
      "Epoch 789/5000, [####--------------------------] Loss: 0.3596, Epsilon: 0.2061, Reward: 0.0\n",
      "Epoch 790/5000, [####--------------------------] Loss: 0.4396, Epsilon: 0.2056, Reward: 0.0\n",
      "Epoch 791/5000, [####--------------------------] Loss: 0.2074, Epsilon: 0.2052, Reward: 0.0\n",
      "Epoch 792/5000, [####--------------------------] Loss: 0.2826, Epsilon: 0.2048, Reward: 0.0\n",
      "Epoch 793/5000, [####--------------------------] Loss: 0.3902, Epsilon: 0.2044, Reward: 800.0\n",
      "Epoch 794/5000, [####--------------------------] Loss: 0.2100, Epsilon: 0.2040, Reward: 0.0\n",
      "Epoch 795/5000, [####--------------------------] Loss: 0.3326, Epsilon: 0.2036, Reward: 0.0\n",
      "Epoch 796/5000, [####--------------------------] Loss: 0.2227, Epsilon: 0.2032, Reward: 0.0\n",
      "Epoch 797/5000, [####--------------------------] Loss: 0.2390, Epsilon: 0.2028, Reward: 0.0\n",
      "Epoch 798/5000, [####--------------------------] Loss: 0.2402, Epsilon: 0.2024, Reward: 0.0\n",
      "Epoch 799/5000, [####--------------------------] Loss: 0.2022, Epsilon: 0.2020, Reward: 0.0\n",
      "Epoch 800/5000, [####--------------------------] Loss: 0.0557, Epsilon: 0.2016, Reward: 0.0\n",
      "Epoch 801/5000, [####--------------------------] Loss: 0.0335, Epsilon: 0.2012, Reward: 0.0\n",
      "Epoch 802/5000, [####--------------------------] Loss: 0.0017, Epsilon: 0.2008, Reward: 0.0\n",
      "Epoch 803/5000, [####--------------------------] Loss: 0.0326, Epsilon: 0.2004, Reward: 0.0\n",
      "Epoch 804/5000, [####--------------------------] Loss: 0.0258, Epsilon: 0.2000, Reward: 1600.0\n",
      "Epoch 805/5000, [####--------------------------] Loss: 0.2769, Epsilon: 0.1996, Reward: 0.0\n",
      "Epoch 806/5000, [####--------------------------] Loss: 0.0791, Epsilon: 0.1992, Reward: 0.0\n",
      "Epoch 807/5000, [####--------------------------] Loss: 0.1504, Epsilon: 0.1988, Reward: 0.0\n",
      "Epoch 808/5000, [####--------------------------] Loss: 0.1118, Epsilon: 0.1984, Reward: 1600.0\n",
      "Epoch 809/5000, [####--------------------------] Loss: 0.0809, Epsilon: 0.1980, Reward: 800.0\n",
      "Epoch 810/5000, [####--------------------------] Loss: 0.2781, Epsilon: 0.1976, Reward: 0.0\n",
      "Epoch 811/5000, [####--------------------------] Loss: 0.3416, Epsilon: 0.1972, Reward: 0.0\n",
      "Epoch 812/5000, [####--------------------------] Loss: 0.1903, Epsilon: 0.1968, Reward: 0.0\n",
      "Epoch 813/5000, [####--------------------------] Loss: 0.3969, Epsilon: 0.1964, Reward: 0.0\n",
      "Epoch 814/5000, [####--------------------------] Loss: 19974.0879, Epsilon: 0.1960, Reward: 0.0\n",
      "Epoch 815/5000, [####--------------------------] Loss: 3.2760, Epsilon: 0.1956, Reward: 0.0\n",
      "Epoch 816/5000, [####--------------------------] Loss: 0.2087, Epsilon: 0.1952, Reward: 0.0\n",
      "Epoch 817/5000, [####--------------------------] Loss: 0.1422, Epsilon: 0.1948, Reward: 0.0\n",
      "Epoch 818/5000, [####--------------------------] Loss: 0.2658, Epsilon: 0.1944, Reward: 0.0\n",
      "Epoch 819/5000, [####--------------------------] Loss: 0.4046, Epsilon: 0.1940, Reward: 0.0\n",
      "Epoch 820/5000, [####--------------------------] Loss: 0.0911, Epsilon: 0.1937, Reward: 0.0\n",
      "Epoch 821/5000, [####--------------------------] Loss: 0.0324, Epsilon: 0.1933, Reward: 800.0\n",
      "Epoch 822/5000, [####--------------------------] Loss: 0.0195, Epsilon: 0.1929, Reward: 0.0\n",
      "Epoch 823/5000, [####--------------------------] Loss: 0.0755, Epsilon: 0.1925, Reward: 1600.0\n",
      "Epoch 824/5000, [####--------------------------] Loss: 0.0922, Epsilon: 0.1921, Reward: 1600.0\n",
      "Epoch 825/5000, [####--------------------------] Loss: 0.2312, Epsilon: 0.1917, Reward: 800.0\n",
      "Epoch 826/5000, [####--------------------------] Loss: 0.4605, Epsilon: 0.1913, Reward: 0.0\n",
      "Epoch 827/5000, [####--------------------------] Loss: 0.7607, Epsilon: 0.1910, Reward: 1600.0\n",
      "Epoch 828/5000, [####--------------------------] Loss: 3.9228, Epsilon: 0.1906, Reward: 0.0\n",
      "Epoch 829/5000, [####--------------------------] Loss: 0.5265, Epsilon: 0.1902, Reward: 0.0\n",
      "Epoch 830/5000, [####--------------------------] Loss: 0.9915, Epsilon: 0.1898, Reward: 2400.0\n",
      "Epoch 831/5000, [####--------------------------] Loss: 1.2206, Epsilon: 0.1894, Reward: 0.0\n",
      "Epoch 832/5000, [####--------------------------] Loss: 19960.9883, Epsilon: 0.1891, Reward: 0.0\n",
      "Epoch 833/5000, [####--------------------------] Loss: 0.9548, Epsilon: 0.1887, Reward: 0.0\n",
      "Epoch 834/5000, [#####-------------------------] Loss: 1.0984, Epsilon: 0.1883, Reward: 800.0\n",
      "Epoch 835/5000, [#####-------------------------] Loss: 1.0549, Epsilon: 0.1879, Reward: 0.0\n",
      "Epoch 836/5000, [#####-------------------------] Loss: 0.4735, Epsilon: 0.1876, Reward: 0.0\n",
      "Epoch 837/5000, [#####-------------------------] Loss: 0.7790, Epsilon: 0.1872, Reward: 0.0\n",
      "Epoch 838/5000, [#####-------------------------] Loss: 0.8296, Epsilon: 0.1868, Reward: 0.0\n",
      "Epoch 839/5000, [#####-------------------------] Loss: 0.6943, Epsilon: 0.1864, Reward: 1600.0\n",
      "Epoch 840/5000, [#####-------------------------] Loss: 0.5438, Epsilon: 0.1861, Reward: 0.0\n",
      "Epoch 841/5000, [#####-------------------------] Loss: 0.7702, Epsilon: 0.1857, Reward: 1600.0\n",
      "Epoch 842/5000, [#####-------------------------] Loss: 0.9132, Epsilon: 0.1853, Reward: 0.0\n",
      "Epoch 843/5000, [#####-------------------------] Loss: 0.7369, Epsilon: 0.1849, Reward: 0.0\n",
      "Epoch 844/5000, [#####-------------------------] Loss: 0.4923, Epsilon: 0.1846, Reward: 800.0\n",
      "Epoch 845/5000, [#####-------------------------] Loss: 4.0164, Epsilon: 0.1842, Reward: 0.0\n",
      "Epoch 846/5000, [#####-------------------------] Loss: 0.9716, Epsilon: 0.1838, Reward: 0.0\n",
      "Epoch 847/5000, [#####-------------------------] Loss: 0.8019, Epsilon: 0.1835, Reward: 0.0\n",
      "Epoch 848/5000, [#####-------------------------] Loss: 0.5196, Epsilon: 0.1831, Reward: 0.0\n",
      "Epoch 849/5000, [#####-------------------------] Loss: 0.4081, Epsilon: 0.1827, Reward: 0.0\n",
      "Epoch 850/5000, [#####-------------------------] Loss: 19953.5547, Epsilon: 0.1824, Reward: 0.0\n",
      "Epoch 851/5000, [#####-------------------------] Loss: 0.3787, Epsilon: 0.1820, Reward: 0.0\n",
      "Epoch 852/5000, [#####-------------------------] Loss: 0.2021, Epsilon: 0.1816, Reward: 800.0\n",
      "Epoch 853/5000, [#####-------------------------] Loss: 0.3456, Epsilon: 0.1813, Reward: 0.0\n",
      "Epoch 854/5000, [#####-------------------------] Loss: 0.4253, Epsilon: 0.1809, Reward: 0.0\n",
      "Epoch 855/5000, [#####-------------------------] Loss: 0.4400, Epsilon: 0.1806, Reward: 1600.0\n",
      "Epoch 856/5000, [#####-------------------------] Loss: 0.1460, Epsilon: 0.1802, Reward: 0.0\n",
      "Epoch 857/5000, [#####-------------------------] Loss: 0.0776, Epsilon: 0.1798, Reward: 0.0\n",
      "Epoch 858/5000, [#####-------------------------] Loss: 0.0568, Epsilon: 0.1795, Reward: 0.0\n",
      "Epoch 859/5000, [#####-------------------------] Loss: 0.0566, Epsilon: 0.1791, Reward: 0.0\n",
      "Epoch 860/5000, [#####-------------------------] Loss: 0.0833, Epsilon: 0.1788, Reward: 0.0\n",
      "Epoch 861/5000, [#####-------------------------] Loss: 0.2019, Epsilon: 0.1784, Reward: 0.0\n",
      "Epoch 862/5000, [#####-------------------------] Loss: 0.0571, Epsilon: 0.1780, Reward: 0.0\n",
      "Epoch 863/5000, [#####-------------------------] Loss: 0.0477, Epsilon: 0.1777, Reward: 0.0\n",
      "Epoch 864/5000, [#####-------------------------] Loss: 0.0188, Epsilon: 0.1773, Reward: 0.0\n",
      "Epoch 865/5000, [#####-------------------------] Loss: 0.0553, Epsilon: 0.1770, Reward: 0.0\n",
      "Epoch 866/5000, [#####-------------------------] Loss: 0.0450, Epsilon: 0.1766, Reward: 1600.0\n",
      "Epoch 867/5000, [#####-------------------------] Loss: 0.0646, Epsilon: 0.1763, Reward: 800.0\n",
      "Epoch 868/5000, [#####-------------------------] Loss: 0.1393, Epsilon: 0.1759, Reward: 0.0\n",
      "Epoch 869/5000, [#####-------------------------] Loss: 19977.8164, Epsilon: 0.1756, Reward: 1600.0\n",
      "Epoch 870/5000, [#####-------------------------] Loss: 0.2322, Epsilon: 0.1752, Reward: 0.0\n",
      "Epoch 871/5000, [#####-------------------------] Loss: 0.3095, Epsilon: 0.1749, Reward: 0.0\n",
      "Epoch 872/5000, [#####-------------------------] Loss: 0.9010, Epsilon: 0.1745, Reward: 1600.0\n",
      "Epoch 873/5000, [#####-------------------------] Loss: 0.6168, Epsilon: 0.1742, Reward: 0.0\n",
      "Epoch 874/5000, [#####-------------------------] Loss: 0.2794, Epsilon: 0.1738, Reward: 0.0\n",
      "Epoch 875/5000, [#####-------------------------] Loss: 0.4522, Epsilon: 0.1735, Reward: 0.0\n",
      "Epoch 876/5000, [#####-------------------------] Loss: 0.2133, Epsilon: 0.1731, Reward: 0.0\n",
      "Epoch 877/5000, [#####-------------------------] Loss: 0.4340, Epsilon: 0.1728, Reward: 0.0\n",
      "Epoch 878/5000, [#####-------------------------] Loss: 0.4261, Epsilon: 0.1724, Reward: 0.0\n",
      "Epoch 879/5000, [#####-------------------------] Loss: 0.2539, Epsilon: 0.1721, Reward: 0.0\n",
      "Epoch 880/5000, [#####-------------------------] Loss: 0.3170, Epsilon: 0.1717, Reward: 0.0\n",
      "Epoch 881/5000, [#####-------------------------] Loss: 0.1909, Epsilon: 0.1714, Reward: 0.0\n",
      "Epoch 882/5000, [#####-------------------------] Loss: 0.1227, Epsilon: 0.1711, Reward: 0.0\n",
      "Epoch 883/5000, [#####-------------------------] Loss: 0.0643, Epsilon: 0.1707, Reward: 0.0\n",
      "Epoch 884/5000, [#####-------------------------] Loss: 0.0851, Epsilon: 0.1704, Reward: 0.0\n",
      "Epoch 885/5000, [#####-------------------------] Loss: 19982.3242, Epsilon: 0.1700, Reward: 0.0\n",
      "Epoch 886/5000, [#####-------------------------] Loss: 0.0401, Epsilon: 0.1697, Reward: 1600.0\n",
      "Epoch 887/5000, [#####-------------------------] Loss: 0.0119, Epsilon: 0.1694, Reward: 0.0\n",
      "Epoch 888/5000, [#####-------------------------] Loss: 0.0007, Epsilon: 0.1690, Reward: 0.0\n",
      "Epoch 889/5000, [#####-------------------------] Loss: 0.0001, Epsilon: 0.1687, Reward: 0.0\n",
      "Epoch 890/5000, [#####-------------------------] Loss: 0.1283, Epsilon: 0.1683, Reward: 0.0\n",
      "Epoch 891/5000, [#####-------------------------] Loss: 0.1930, Epsilon: 0.1680, Reward: 0.0\n",
      "Epoch 892/5000, [#####-------------------------] Loss: 0.0283, Epsilon: 0.1677, Reward: 0.0\n",
      "Epoch 893/5000, [#####-------------------------] Loss: 0.0037, Epsilon: 0.1673, Reward: 0.0\n",
      "Epoch 894/5000, [#####-------------------------] Loss: 0.0070, Epsilon: 0.1670, Reward: 0.0\n",
      "Epoch 895/5000, [#####-------------------------] Loss: 0.0122, Epsilon: 0.1667, Reward: 0.0\n",
      "Epoch 896/5000, [#####-------------------------] Loss: 0.0043, Epsilon: 0.1663, Reward: 0.0\n",
      "Epoch 897/5000, [#####-------------------------] Loss: 0.0008, Epsilon: 0.1660, Reward: 0.0\n",
      "Epoch 898/5000, [#####-------------------------] Loss: 0.1009, Epsilon: 0.1657, Reward: 0.0\n",
      "Epoch 899/5000, [#####-------------------------] Loss: 2.9950, Epsilon: 0.1653, Reward: 0.0\n",
      "Epoch 900/5000, [#####-------------------------] Loss: 0.0356, Epsilon: 0.1650, Reward: 0.0\n",
      "Epoch 901/5000, [#####-------------------------] Loss: 0.0019, Epsilon: 0.1647, Reward: 0.0\n",
      "Epoch 902/5000, [#####-------------------------] Loss: 0.0012, Epsilon: 0.1643, Reward: 0.0\n",
      "Epoch 903/5000, [#####-------------------------] Loss: 0.0018, Epsilon: 0.1640, Reward: 0.0\n",
      "Epoch 904/5000, [#####-------------------------] Loss: 0.0018, Epsilon: 0.1637, Reward: 0.0\n",
      "Epoch 905/5000, [#####-------------------------] Loss: 0.0001, Epsilon: 0.1634, Reward: 0.0\n",
      "Epoch 906/5000, [#####-------------------------] Loss: 0.0005, Epsilon: 0.1630, Reward: 0.0\n",
      "Epoch 907/5000, [#####-------------------------] Loss: 0.0048, Epsilon: 0.1627, Reward: 0.0\n",
      "Epoch 908/5000, [#####-------------------------] Loss: 0.0017, Epsilon: 0.1624, Reward: 1600.0\n",
      "Epoch 909/5000, [#####-------------------------] Loss: 0.0865, Epsilon: 0.1621, Reward: 800.0\n",
      "Epoch 910/5000, [#####-------------------------] Loss: 0.1594, Epsilon: 0.1617, Reward: 0.0\n",
      "Epoch 911/5000, [#####-------------------------] Loss: 0.1268, Epsilon: 0.1614, Reward: 0.0\n",
      "Epoch 912/5000, [#####-------------------------] Loss: 0.1529, Epsilon: 0.1611, Reward: 1600.0\n",
      "Epoch 913/5000, [#####-------------------------] Loss: 0.9193, Epsilon: 0.1608, Reward: 2400.0\n",
      "Epoch 914/5000, [#####-------------------------] Loss: 1.6279, Epsilon: 0.1604, Reward: 0.0\n",
      "Epoch 915/5000, [#####-------------------------] Loss: 0.7774, Epsilon: 0.1601, Reward: 0.0\n",
      "Epoch 916/5000, [#####-------------------------] Loss: 0.4588, Epsilon: 0.1598, Reward: 1600.0\n",
      "Epoch 917/5000, [#####-------------------------] Loss: 3.6093, Epsilon: 0.1595, Reward: 0.0\n",
      "Epoch 918/5000, [#####-------------------------] Loss: 0.5852, Epsilon: 0.1592, Reward: 0.0\n",
      "Epoch 919/5000, [#####-------------------------] Loss: 0.4574, Epsilon: 0.1588, Reward: 1600.0\n",
      "Epoch 920/5000, [#####-------------------------] Loss: 19951.6738, Epsilon: 0.1585, Reward: 0.0\n",
      "Epoch 921/5000, [#####-------------------------] Loss: 1.4128, Epsilon: 0.1582, Reward: 0.0\n",
      "Epoch 922/5000, [#####-------------------------] Loss: 1.0861, Epsilon: 0.1579, Reward: 1600.0\n",
      "Epoch 923/5000, [#####-------------------------] Loss: 0.9898, Epsilon: 0.1576, Reward: 1600.0\n",
      "Epoch 924/5000, [#####-------------------------] Loss: 0.6220, Epsilon: 0.1573, Reward: 0.0\n",
      "Epoch 925/5000, [#####-------------------------] Loss: 1.5050, Epsilon: 0.1569, Reward: 1600.0\n",
      "Epoch 926/5000, [#####-------------------------] Loss: 19947.0645, Epsilon: 0.1566, Reward: 0.0\n",
      "Epoch 927/5000, [#####-------------------------] Loss: 0.7983, Epsilon: 0.1563, Reward: 0.0\n",
      "Epoch 928/5000, [#####-------------------------] Loss: 0.7067, Epsilon: 0.1560, Reward: 1600.0\n",
      "Epoch 929/5000, [#####-------------------------] Loss: 1.1606, Epsilon: 0.1557, Reward: 0.0\n",
      "Epoch 930/5000, [#####-------------------------] Loss: 0.9733, Epsilon: 0.1554, Reward: 0.0\n",
      "Epoch 931/5000, [#####-------------------------] Loss: 1.2102, Epsilon: 0.1551, Reward: 0.0\n",
      "Epoch 932/5000, [#####-------------------------] Loss: 0.9193, Epsilon: 0.1548, Reward: 1600.0\n",
      "Epoch 933/5000, [#####-------------------------] Loss: 0.6198, Epsilon: 0.1545, Reward: 0.0\n",
      "Epoch 934/5000, [#####-------------------------] Loss: 0.7455, Epsilon: 0.1541, Reward: 0.0\n",
      "Epoch 935/5000, [#####-------------------------] Loss: 0.3820, Epsilon: 0.1538, Reward: 0.0\n",
      "Epoch 936/5000, [#####-------------------------] Loss: 0.4338, Epsilon: 0.1535, Reward: 2400.0\n",
      "Epoch 937/5000, [#####-------------------------] Loss: 0.5423, Epsilon: 0.1532, Reward: 0.0\n",
      "Epoch 938/5000, [#####-------------------------] Loss: 0.4392, Epsilon: 0.1529, Reward: 0.0\n",
      "Epoch 939/5000, [#####-------------------------] Loss: 0.2978, Epsilon: 0.1526, Reward: 0.0\n",
      "Epoch 940/5000, [#####-------------------------] Loss: 0.2894, Epsilon: 0.1523, Reward: 0.0\n",
      "Epoch 941/5000, [#####-------------------------] Loss: 0.4845, Epsilon: 0.1520, Reward: 0.0\n",
      "Epoch 942/5000, [#####-------------------------] Loss: 0.1720, Epsilon: 0.1517, Reward: 0.0\n",
      "Epoch 943/5000, [#####-------------------------] Loss: 0.3232, Epsilon: 0.1514, Reward: 1600.0\n",
      "Epoch 944/5000, [#####-------------------------] Loss: 0.4641, Epsilon: 0.1511, Reward: 0.0\n",
      "Epoch 945/5000, [#####-------------------------] Loss: 0.2949, Epsilon: 0.1508, Reward: 0.0\n",
      "Epoch 946/5000, [#####-------------------------] Loss: 0.1074, Epsilon: 0.1505, Reward: 0.0\n",
      "Epoch 947/5000, [#####-------------------------] Loss: 0.1981, Epsilon: 0.1502, Reward: 0.0\n",
      "Epoch 948/5000, [#####-------------------------] Loss: 19988.8242, Epsilon: 0.1499, Reward: 0.0\n",
      "Epoch 949/5000, [#####-------------------------] Loss: 3.5145, Epsilon: 0.1496, Reward: 0.0\n",
      "Epoch 950/5000, [#####-------------------------] Loss: 0.0897, Epsilon: 0.1493, Reward: 1600.0\n",
      "Epoch 951/5000, [#####-------------------------] Loss: 0.0782, Epsilon: 0.1490, Reward: 1600.0\n",
      "Epoch 952/5000, [#####-------------------------] Loss: 0.3655, Epsilon: 0.1487, Reward: 0.0\n",
      "Epoch 953/5000, [#####-------------------------] Loss: 0.4124, Epsilon: 0.1484, Reward: 1600.0\n",
      "Epoch 954/5000, [#####-------------------------] Loss: 0.4684, Epsilon: 0.1481, Reward: 0.0\n",
      "Epoch 955/5000, [#####-------------------------] Loss: 0.5756, Epsilon: 0.1478, Reward: 0.0\n",
      "Epoch 956/5000, [#####-------------------------] Loss: 0.5104, Epsilon: 0.1475, Reward: 0.0\n",
      "Epoch 957/5000, [#####-------------------------] Loss: 0.4877, Epsilon: 0.1472, Reward: 0.0\n",
      "Epoch 958/5000, [#####-------------------------] Loss: 0.4610, Epsilon: 0.1469, Reward: 0.0\n",
      "Epoch 959/5000, [#####-------------------------] Loss: 0.2435, Epsilon: 0.1466, Reward: 0.0\n",
      "Epoch 960/5000, [#####-------------------------] Loss: 0.2495, Epsilon: 0.1463, Reward: 0.0\n",
      "Epoch 961/5000, [#####-------------------------] Loss: 0.4230, Epsilon: 0.1460, Reward: 1600.0\n",
      "Epoch 962/5000, [#####-------------------------] Loss: 0.3186, Epsilon: 0.1457, Reward: 0.0\n",
      "Epoch 963/5000, [#####-------------------------] Loss: 0.4801, Epsilon: 0.1454, Reward: 0.0\n",
      "Epoch 964/5000, [#####-------------------------] Loss: 0.5243, Epsilon: 0.1452, Reward: 0.0\n",
      "Epoch 965/5000, [#####-------------------------] Loss: 0.1895, Epsilon: 0.1449, Reward: 0.0\n",
      "Epoch 966/5000, [#####-------------------------] Loss: 0.2567, Epsilon: 0.1446, Reward: 0.0\n",
      "Epoch 967/5000, [#####-------------------------] Loss: 3.4216, Epsilon: 0.1443, Reward: 0.0\n",
      "Epoch 968/5000, [#####-------------------------] Loss: 0.1467, Epsilon: 0.1440, Reward: 0.0\n",
      "Epoch 969/5000, [#####-------------------------] Loss: 0.1606, Epsilon: 0.1437, Reward: 0.0\n",
      "Epoch 970/5000, [#####-------------------------] Loss: 0.0102, Epsilon: 0.1434, Reward: 0.0\n",
      "Epoch 971/5000, [#####-------------------------] Loss: 0.0288, Epsilon: 0.1431, Reward: 0.0\n",
      "Epoch 972/5000, [#####-------------------------] Loss: 0.0482, Epsilon: 0.1429, Reward: 0.0\n",
      "Epoch 973/5000, [#####-------------------------] Loss: 0.1018, Epsilon: 0.1426, Reward: 0.0\n",
      "Epoch 974/5000, [#####-------------------------] Loss: 0.0639, Epsilon: 0.1423, Reward: 0.0\n",
      "Epoch 975/5000, [#####-------------------------] Loss: 0.0200, Epsilon: 0.1420, Reward: 0.0\n",
      "Epoch 976/5000, [#####-------------------------] Loss: 0.0383, Epsilon: 0.1417, Reward: 0.0\n",
      "Epoch 977/5000, [#####-------------------------] Loss: 0.0460, Epsilon: 0.1414, Reward: 0.0\n",
      "Epoch 978/5000, [#####-------------------------] Loss: 0.0017, Epsilon: 0.1411, Reward: 0.0\n",
      "Epoch 979/5000, [#####-------------------------] Loss: 0.0003, Epsilon: 0.1409, Reward: 1600.0\n",
      "Epoch 980/5000, [#####-------------------------] Loss: 0.3315, Epsilon: 0.1406, Reward: 1600.0\n",
      "Epoch 981/5000, [#####-------------------------] Loss: 1.4499, Epsilon: 0.1403, Reward: 1600.0\n",
      "Epoch 982/5000, [#####-------------------------] Loss: 0.4748, Epsilon: 0.1400, Reward: 0.0\n",
      "Epoch 983/5000, [#####-------------------------] Loss: 0.4841, Epsilon: 0.1397, Reward: 0.0\n",
      "Epoch 984/5000, [#####-------------------------] Loss: 0.7499, Epsilon: 0.1395, Reward: 0.0\n",
      "Epoch 985/5000, [#####-------------------------] Loss: 19961.3320, Epsilon: 0.1392, Reward: 0.0\n",
      "Epoch 986/5000, [#####-------------------------] Loss: 0.1570, Epsilon: 0.1389, Reward: 0.0\n",
      "Epoch 987/5000, [#####-------------------------] Loss: 0.6786, Epsilon: 0.1386, Reward: 0.0\n",
      "Epoch 988/5000, [#####-------------------------] Loss: 0.5168, Epsilon: 0.1383, Reward: 0.0\n",
      "Epoch 989/5000, [#####-------------------------] Loss: 0.4243, Epsilon: 0.1381, Reward: 0.0\n",
      "Epoch 990/5000, [#####-------------------------] Loss: 0.0341, Epsilon: 0.1378, Reward: 0.0\n",
      "Epoch 991/5000, [#####-------------------------] Loss: 0.0035, Epsilon: 0.1375, Reward: 0.0\n",
      "Epoch 992/5000, [#####-------------------------] Loss: 0.0022, Epsilon: 0.1372, Reward: 0.0\n",
      "Epoch 993/5000, [#####-------------------------] Loss: 0.0001, Epsilon: 0.1370, Reward: 0.0\n",
      "Epoch 994/5000, [#####-------------------------] Loss: 0.0007, Epsilon: 0.1367, Reward: 0.0\n",
      "Epoch 995/5000, [#####-------------------------] Loss: 0.0000, Epsilon: 0.1364, Reward: 0.0\n",
      "Epoch 996/5000, [#####-------------------------] Loss: 0.0001, Epsilon: 0.1362, Reward: 0.0\n",
      "Epoch 997/5000, [#####-------------------------] Loss: 0.0250, Epsilon: 0.1359, Reward: 1600.0\n",
      "Epoch 998/5000, [#####-------------------------] Loss: 0.0599, Epsilon: 0.1356, Reward: 0.0\n",
      "Epoch 999/5000, [#####-------------------------] Loss: 0.0671, Epsilon: 0.1353, Reward: 0.0\n",
      "Epoch 1000/5000, [######------------------------] Loss: 0.0406, Epsilon: 0.1351, Reward: 0.0\n",
      "Epoch 1001/5000, [######------------------------] Loss: 0.0105, Epsilon: 0.1348, Reward: 0.0\n",
      "Epoch 1002/5000, [######------------------------] Loss: 0.0599, Epsilon: 0.1345, Reward: 1600.0\n",
      "Epoch 1003/5000, [######------------------------] Loss: 0.0756, Epsilon: 0.1343, Reward: 0.0\n",
      "Epoch 1004/5000, [######------------------------] Loss: 0.0299, Epsilon: 0.1340, Reward: 0.0\n",
      "Epoch 1005/5000, [######------------------------] Loss: 0.0305, Epsilon: 0.1337, Reward: 0.0\n",
      "Epoch 1006/5000, [######------------------------] Loss: 0.0115, Epsilon: 0.1335, Reward: 0.0\n",
      "Epoch 1007/5000, [######------------------------] Loss: 3.9007, Epsilon: 0.1332, Reward: 0.0\n",
      "Epoch 1008/5000, [######------------------------] Loss: 0.3716, Epsilon: 0.1329, Reward: 0.0\n",
      "Epoch 1009/5000, [######------------------------] Loss: 3.5991, Epsilon: 0.1327, Reward: 0.0\n",
      "Epoch 1010/5000, [######------------------------] Loss: 0.1377, Epsilon: 0.1324, Reward: 0.0\n",
      "Epoch 1011/5000, [######------------------------] Loss: 0.0461, Epsilon: 0.1321, Reward: 0.0\n",
      "Epoch 1012/5000, [######------------------------] Loss: 0.0696, Epsilon: 0.1319, Reward: 0.0\n",
      "Epoch 1013/5000, [######------------------------] Loss: 0.0642, Epsilon: 0.1316, Reward: 0.0\n",
      "Epoch 1014/5000, [######------------------------] Loss: 0.0620, Epsilon: 0.1313, Reward: 0.0\n",
      "Epoch 1015/5000, [######------------------------] Loss: 0.0332, Epsilon: 0.1311, Reward: 0.0\n",
      "Epoch 1016/5000, [######------------------------] Loss: 3.2910, Epsilon: 0.1308, Reward: 0.0\n",
      "Epoch 1017/5000, [######------------------------] Loss: 0.0016, Epsilon: 0.1305, Reward: 0.0\n",
      "Epoch 1018/5000, [######------------------------] Loss: 0.0001, Epsilon: 0.1303, Reward: 0.0\n",
      "Epoch 1019/5000, [######------------------------] Loss: 0.0000, Epsilon: 0.1300, Reward: 1600.0\n",
      "Epoch 1020/5000, [######------------------------] Loss: 0.0968, Epsilon: 0.1298, Reward: 800.0\n",
      "Epoch 1021/5000, [######------------------------] Loss: 0.2693, Epsilon: 0.1295, Reward: 800.0\n",
      "Epoch 1022/5000, [######------------------------] Loss: 0.2422, Epsilon: 0.1292, Reward: 0.0\n",
      "Epoch 1023/5000, [######------------------------] Loss: 0.9986, Epsilon: 0.1290, Reward: 0.0\n",
      "Epoch 1024/5000, [######------------------------] Loss: 0.5296, Epsilon: 0.1287, Reward: 0.0\n",
      "Epoch 1025/5000, [######------------------------] Loss: 0.2472, Epsilon: 0.1285, Reward: 0.0\n",
      "Epoch 1026/5000, [######------------------------] Loss: 0.1898, Epsilon: 0.1282, Reward: 0.0\n",
      "Epoch 1027/5000, [######------------------------] Loss: 0.2652, Epsilon: 0.1280, Reward: 0.0\n",
      "Epoch 1028/5000, [######------------------------] Loss: 0.1014, Epsilon: 0.1277, Reward: 0.0\n",
      "Epoch 1029/5000, [######------------------------] Loss: 0.2297, Epsilon: 0.1274, Reward: 0.0\n",
      "Epoch 1030/5000, [######------------------------] Loss: 0.0854, Epsilon: 0.1272, Reward: 0.0\n",
      "Epoch 1031/5000, [######------------------------] Loss: 0.0795, Epsilon: 0.1269, Reward: 0.0\n",
      "Epoch 1032/5000, [######------------------------] Loss: 0.0074, Epsilon: 0.1267, Reward: 0.0\n",
      "Epoch 1033/5000, [######------------------------] Loss: 0.0000, Epsilon: 0.1264, Reward: 0.0\n",
      "Epoch 1034/5000, [######------------------------] Loss: 0.0004, Epsilon: 0.1262, Reward: 0.0\n",
      "Epoch 1035/5000, [######------------------------] Loss: 0.0004, Epsilon: 0.1259, Reward: 0.0\n",
      "Epoch 1036/5000, [######------------------------] Loss: 0.0014, Epsilon: 0.1257, Reward: 0.0\n",
      "Epoch 1037/5000, [######------------------------] Loss: 0.0001, Epsilon: 0.1254, Reward: 0.0\n",
      "Epoch 1038/5000, [######------------------------] Loss: 0.0015, Epsilon: 0.1252, Reward: 0.0\n",
      "Epoch 1039/5000, [######------------------------] Loss: 0.0000, Epsilon: 0.1249, Reward: 0.0\n",
      "Epoch 1040/5000, [######------------------------] Loss: 3.2187, Epsilon: 0.1247, Reward: 0.0\n",
      "Epoch 1041/5000, [######------------------------] Loss: 0.0004, Epsilon: 0.1244, Reward: 0.0\n",
      "Epoch 1042/5000, [######------------------------] Loss: 0.0026, Epsilon: 0.1242, Reward: 0.0\n",
      "Epoch 1043/5000, [######------------------------] Loss: 0.0000, Epsilon: 0.1239, Reward: 0.0\n",
      "Epoch 1044/5000, [######------------------------] Loss: 0.0004, Epsilon: 0.1237, Reward: 0.0\n",
      "Epoch 1045/5000, [######------------------------] Loss: 0.0014, Epsilon: 0.1234, Reward: 0.0\n",
      "Epoch 1046/5000, [######------------------------] Loss: 0.0000, Epsilon: 0.1232, Reward: 0.0\n",
      "Epoch 1047/5000, [######------------------------] Loss: 0.0002, Epsilon: 0.1229, Reward: 0.0\n",
      "Epoch 1048/5000, [######------------------------] Loss: 0.0000, Epsilon: 0.1227, Reward: 0.0\n",
      "Epoch 1049/5000, [######------------------------] Loss: 0.0017, Epsilon: 0.1224, Reward: 0.0\n",
      "Epoch 1050/5000, [######------------------------] Loss: 0.0114, Epsilon: 0.1222, Reward: 2400.0\n",
      "Epoch 1051/5000, [######------------------------] Loss: 0.2679, Epsilon: 0.1220, Reward: 0.0\n",
      "Epoch 1052/5000, [######------------------------] Loss: 0.6202, Epsilon: 0.1217, Reward: 0.0\n",
      "Epoch 1053/5000, [######------------------------] Loss: 0.5088, Epsilon: 0.1215, Reward: 0.0\n",
      "Epoch 1054/5000, [######------------------------] Loss: 0.5507, Epsilon: 0.1212, Reward: 0.0\n",
      "Epoch 1055/5000, [######------------------------] Loss: 0.2405, Epsilon: 0.1210, Reward: 0.0\n",
      "Epoch 1056/5000, [######------------------------] Loss: 0.0349, Epsilon: 0.1207, Reward: 800.0\n",
      "Epoch 1057/5000, [######------------------------] Loss: 0.0949, Epsilon: 0.1205, Reward: 0.0\n",
      "Epoch 1058/5000, [######------------------------] Loss: 0.0653, Epsilon: 0.1203, Reward: 0.0\n",
      "Epoch 1059/5000, [######------------------------] Loss: 0.1443, Epsilon: 0.1200, Reward: 0.0\n",
      "Epoch 1060/5000, [######------------------------] Loss: 0.0597, Epsilon: 0.1198, Reward: 0.0\n",
      "Epoch 1061/5000, [######------------------------] Loss: 0.0370, Epsilon: 0.1195, Reward: 800.0\n",
      "Epoch 1062/5000, [######------------------------] Loss: 0.1271, Epsilon: 0.1193, Reward: 0.0\n",
      "Epoch 1063/5000, [######------------------------] Loss: 0.1040, Epsilon: 0.1191, Reward: 0.0\n",
      "Epoch 1064/5000, [######------------------------] Loss: 0.6079, Epsilon: 0.1188, Reward: 800.0\n",
      "Epoch 1065/5000, [######------------------------] Loss: 0.2071, Epsilon: 0.1186, Reward: 0.0\n",
      "Epoch 1066/5000, [######------------------------] Loss: 0.1062, Epsilon: 0.1183, Reward: 0.0\n",
      "Epoch 1067/5000, [######------------------------] Loss: 0.0042, Epsilon: 0.1181, Reward: 0.0\n",
      "Epoch 1068/5000, [######------------------------] Loss: 0.0496, Epsilon: 0.1179, Reward: 0.0\n",
      "Epoch 1069/5000, [######------------------------] Loss: 0.0361, Epsilon: 0.1176, Reward: 0.0\n",
      "Epoch 1070/5000, [######------------------------] Loss: 0.0288, Epsilon: 0.1174, Reward: 0.0\n",
      "Epoch 1071/5000, [######------------------------] Loss: 0.0305, Epsilon: 0.1172, Reward: 0.0\n",
      "Epoch 1072/5000, [######------------------------] Loss: 0.0207, Epsilon: 0.1169, Reward: 0.0\n",
      "Epoch 1073/5000, [######------------------------] Loss: 0.0533, Epsilon: 0.1167, Reward: 0.0\n",
      "Epoch 1074/5000, [######------------------------] Loss: 0.0494, Epsilon: 0.1165, Reward: 1600.0\n",
      "Epoch 1075/5000, [######------------------------] Loss: 0.1807, Epsilon: 0.1162, Reward: 800.0\n",
      "Epoch 1076/5000, [######------------------------] Loss: 0.0410, Epsilon: 0.1160, Reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQN(env, in_channels, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m dqn\u001b[38;5;241m.\u001b[39m_train(env, num_epochs \u001b[38;5;241m=\u001b[39m epochs)\n",
      "File \u001b[0;32m~/Desktop/Projects/dqn-mario/dqn.py:168\u001b[0m, in \u001b[0;36mDQN._train\u001b[0;34m(self, env, num_epochs, batch_size, gamma)\u001b[0m\n\u001b[1;32m    162\u001b[0m done_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch\u001b[38;5;241m.\u001b[39mdone, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# print(f'State batch shape: {state_batch.shape}')\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# print(f'Action batch shape: {action_batch.shape}')\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# compute Q-values from current model\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(state_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# compute target Q-values from target network\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Projects/dqn-mario/dqn.py:92\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    input: x = np array returned from env.render() method\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    return: the q-value associated with this episode\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers(x)\n\u001b[1;32m     93\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Projects/dqn-mario/cnn.py:17\u001b[0m, in \u001b[0;36mConvFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "obs, info = env.reset()\n",
    "in_channels = env.render().shape[2]\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# init wandb logging \n",
    "wandb.init(\n",
    "    entity=\"zavierand-new-york-university\",\n",
    "    project='dqn-mario',\n",
    "    config={\n",
    "        'learning_rate': 1e-2,\n",
    "        'architecture': 'DQN',\n",
    "        'epochs': 5000,\n",
    "    },\n",
    ")\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "# instantiate the model\n",
    "dqn = DQN(env, in_channels, env.action_space.n)\n",
    "\n",
    "# train the model\n",
    "# dqn._train(env, num_epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4cbea1-7c26-43b1-8e80-032d31c940f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward per Episode: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the trained model\n",
    "checkpoint = torch.load('./trained_models/dqn_checkpoint_epoch_1000.pt')\n",
    "dqn.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate the model's performance and capture frames\n",
    "num_episodes = 100\n",
    "avg_reward, rewards, frames = dqn._eval(env, num_episodes, render=True)\n",
    "\n",
    "# plot rewards per episode\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Rewards per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a846cef2-e6fb-440a-88fb-38257bb78cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# animate model's performance for episodes that are a multiple of 1000\n",
    "for frame in frames:\n",
    "    display.display(Image.fromarray(frame))\n",
    "    time.sleep(0.05)  # Adjust for speed of rendering\n",
    "    display.clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
